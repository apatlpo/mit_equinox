{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# drifter processing step 0: from netcdf to pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask.bag as db\n",
    "#import pandas as pd\n",
    "#import geopandas\n",
    "#import datetime\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "#import cartopy.crs as ccrs\n",
    "#import cartopy.feature as cfeature\n",
    "\n",
    "import mitequinox.drifters as edr\n",
    "\n",
    "data_dir = '/work/ALT/swot/aval/syn/drifters/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mp/pontea/.conda/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/config.py:12: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  defaults = yaml.load(f)\n",
      "/home/mp/pontea/.conda/envs/equinox/lib/python3.6/site-packages/distributed/deploy/local.py:138: UserWarning: diagnostics_port has been deprecated. Please use `dashboard_address=` instead\n",
      "  \"diagnostics_port has been deprecated. \"\n"
     ]
    }
   ],
   "source": [
    "from dask_jobqueue import PBSCluster\n",
    "#cluster = PBSCluster(cores=24, processes=12, walltime='12:00:00') #, memory='20GB')\n",
    "cluster = PBSCluster(cores=12, processes=6, walltime='12:00:00') #, memory='20GB')\n",
    "w = cluster.scale(40)\n",
    "#w = cluster.scale(10) # postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dask handles and check dask server status\n",
    "from dask.distributed import Client\n",
    "client = Client(cluster)\n",
    "#client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe6ec1e71ee428ea2e6d28c916b9628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>PBSCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n    .dâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#client\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unknown job_id: 5313904 for worker tcp://10.135.36.90:33191\n",
      "Unknown job_id: 5313902 for worker tcp://10.135.36.90:34790\n",
      "Unknown job_id: 5313903 for worker tcp://10.135.36.90:42067\n",
      "Unknown job_id: 5313905 for worker tcp://10.135.36.92:44369\n",
      "Unknown job_id: 5313895 for worker tcp://10.135.36.85:38318\n",
      "Unknown job_id: 5313894 for worker tcp://10.135.36.85:45128\n",
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n"
     ]
    }
   ],
   "source": [
    "#client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trajectory(ij, ncfile, gps):\n",
    "    ds = xr.open_dataset(ncfile).isel(TIME=slice(ij[0],ij[1]-1))\n",
    "    ds['GPS'] = (gps + ds.U*0.).astype(int)\n",
    "    df = ds.to_dataframe()\n",
    "    id = int(df['ID'][0])\n",
    "    df = df.drop(columns='ID')\n",
    "    return df, id\n",
    "\n",
    "def store(d, gps):\n",
    "    if gps==1:\n",
    "        file = data_dir+'single/gps_%09d.p' %d[1]\n",
    "    else:\n",
    "        file = data_dir+'single/argos_%09d.p' %d[1]        \n",
    "    pickle.dump( d, open( file, 'wb' ) , protocol=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## rewrite hourly data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inspect single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (TIME: 15230718)\n",
      "Coordinates:\n",
      "  * TIME     (TIME) datetime64[ns] 2005-04-15T20:00:00 ... NaT\n",
      "Data variables:\n",
      "    ID       (TIME) float64 ...\n",
      "    LAT      (TIME) float64 ...\n",
      "    LON      (TIME) float64 ...\n",
      "    U        (TIME) float64 ...\n",
      "    V        (TIME) float64 ...\n",
      "    LAT_ERR  (TIME) float64 ...\n",
      "    LON_ERR  (TIME) float64 ...\n",
      "    U_ERR    (TIME) float64 ...\n",
      "    V_ERR    (TIME) float64 ...\n",
      "    GAP      (TIME) timedelta64[ns] ...\n",
      "    RMSGAP   (TIME) timedelta64[ns] ...\n",
      "    DROGUE   (TIME) float64 ...\n",
      "Attributes:\n",
      "    title:        Hourly Argos-tracked drifters location and velocity estimat...\n",
      "    description:  This is version 1.02, block 1 of the dataset. See http://ww...\n",
      "    note:         For all variables of dimension TIME, interruptions in the e...\n",
      "    creator:      Shane Elipot and Rick Lumpkin\n",
      "    timestamp:    19-Dec-2018 13:25:49\n",
      "For all variables of dimension TIME, interruptions in the estimation along a single trajectory are indicated by \"Inf\" values; Individual trajectories are separated by \"NaN\" values; Thus, one can use the COL2CELL function of the JLAB Matlab toolbox (http://www.jmlilly.net) to convert each data matrix into a cell array with one component for each individual trajectory, without the need to load and loop for individual IDs, e.g. lat = col2cell(lat);\n"
     ]
    }
   ],
   "source": [
    "ncfile = data_dir+'raw/driftertrajWMLE_1.02_block1.nc'\n",
    "ds = xr.open_dataset(ncfile)\n",
    "print(ds)\n",
    "print(ds.note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>U</th>\n",
       "      <th>V</th>\n",
       "      <th>LAT_ERR</th>\n",
       "      <th>LON_ERR</th>\n",
       "      <th>U_ERR</th>\n",
       "      <th>V_ERR</th>\n",
       "      <th>GAP</th>\n",
       "      <th>RMSGAP</th>\n",
       "      <th>DROGUE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TIME</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-04-15 20:00:00</th>\n",
       "      <td>47.47501</td>\n",
       "      <td>312.05118</td>\n",
       "      <td>-0.1733</td>\n",
       "      <td>0.2009</td>\n",
       "      <td>0.01129</td>\n",
       "      <td>0.04254</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>00:54:43.200000</td>\n",
       "      <td>01:08:24</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-04-15 21:00:00</th>\n",
       "      <td>47.46839</td>\n",
       "      <td>312.04200</td>\n",
       "      <td>-0.1616</td>\n",
       "      <td>-0.5554</td>\n",
       "      <td>0.00987</td>\n",
       "      <td>0.01140</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>01:04:48</td>\n",
       "      <td>00:58:12</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-04-15 22:00:00</th>\n",
       "      <td>47.44858</td>\n",
       "      <td>312.03870</td>\n",
       "      <td>0.8653</td>\n",
       "      <td>-0.5284</td>\n",
       "      <td>0.00537</td>\n",
       "      <td>0.01501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>00:33:07.200000</td>\n",
       "      <td>00:47:38.400000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-04-15 23:00:00</th>\n",
       "      <td>47.43282</td>\n",
       "      <td>312.07433</td>\n",
       "      <td>0.4313</td>\n",
       "      <td>-0.4776</td>\n",
       "      <td>0.00058</td>\n",
       "      <td>0.00803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>00:28:48</td>\n",
       "      <td>00:49:33.600000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-04-16 00:00:00</th>\n",
       "      <td>47.41716</td>\n",
       "      <td>312.07013</td>\n",
       "      <td>0.0575</td>\n",
       "      <td>-0.4887</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>0.00729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>01:12:00</td>\n",
       "      <td>00:57:18</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          LAT        LON       U       V  LAT_ERR  LON_ERR  \\\n",
       "TIME                                                                         \n",
       "2005-04-15 20:00:00  47.47501  312.05118 -0.1733  0.2009  0.01129  0.04254   \n",
       "2005-04-15 21:00:00  47.46839  312.04200 -0.1616 -0.5554  0.00987  0.01140   \n",
       "2005-04-15 22:00:00  47.44858  312.03870  0.8653 -0.5284  0.00537  0.01501   \n",
       "2005-04-15 23:00:00  47.43282  312.07433  0.4313 -0.4776  0.00058  0.00803   \n",
       "2005-04-16 00:00:00  47.41716  312.07013  0.0575 -0.4887  0.00049  0.00729   \n",
       "\n",
       "                     U_ERR  V_ERR             GAP          RMSGAP  DROGUE  \n",
       "TIME                                                                       \n",
       "2005-04-15 20:00:00    0.0    0.0 00:54:43.200000        01:08:24     1.0  \n",
       "2005-04-15 21:00:00    0.0    0.0        01:04:48        00:58:12     1.0  \n",
       "2005-04-15 22:00:00    0.0    0.0 00:33:07.200000 00:47:38.400000     1.0  \n",
       "2005-04-15 23:00:00    0.0    0.0        00:28:48 00:49:33.600000     1.0  \n",
       "2005-04-16 00:00:00    0.0    0.0        01:12:00        00:57:18     1.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inan = np.where(np.isnan(ds.LAT.values))[0]\n",
    "b = (db.from_sequence([(i+1,j) for i,j in zip(np.hstack([-1,inan])[:-1],inan)], \n",
    "                      npartitions=1000)\n",
    "     .map(load_trajectory, ncfile))\n",
    "b.take(1)[0][0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process all nc block files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/ALT/swot/aval/syn/drifters/raw/driftertrajGPS_1.02.nc\n"
     ]
    }
   ],
   "source": [
    "gps=1\n",
    "\n",
    "ncfile = data_dir+'raw/driftertrajGPS_1.02.nc'\n",
    "#\n",
    "ds = xr.open_dataset(ncfile)\n",
    "inan = np.where(np.isnan(ds.LAT.values))[0]\n",
    "#\n",
    "b = (db.from_sequence([(i+1,j) for i,j in zip(np.hstack([-1,inan])[:-1],inan)], \n",
    "                      npartitions=1000)\n",
    "     .map(load_trajectory, ncfile, gps))\n",
    "b.map(store, gps).compute()\n",
    "print(ncfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/ALT/swot/aval/syn/drifters/raw/driftertrajWMLE_1.02_block1.nc\n",
      "/work/ALT/swot/aval/syn/drifters/raw/driftertrajWMLE_1.02_block2.nc\n",
      "/work/ALT/swot/aval/syn/drifters/raw/driftertrajWMLE_1.02_block3.nc\n",
      "/work/ALT/swot/aval/syn/drifters/raw/driftertrajWMLE_1.02_block4.nc\n",
      "/work/ALT/swot/aval/syn/drifters/raw/driftertrajWMLE_1.02_block5.nc\n",
      "/work/ALT/swot/aval/syn/drifters/raw/driftertrajWMLE_1.02_block6.nc\n",
      "/work/ALT/swot/aval/syn/drifters/raw/driftertrajWMLE_1.02_block7.nc\n"
     ]
    }
   ],
   "source": [
    "gps=0\n",
    "for ifile in range(1,8):\n",
    "    ncfile = data_dir+'raw/driftertrajWMLE_1.02_block%d.nc' %ifile\n",
    "    #\n",
    "    ds = xr.open_dataset(ncfile)\n",
    "    inan = np.where(np.isnan(ds.LAT.values))[0]\n",
    "    #\n",
    "    b = (db.from_sequence([(i+1,j) for i,j in zip(np.hstack([-1,inan])[:-1],inan)], \n",
    "                          npartitions=1000)\n",
    "         .map(load_trajectory, ncfile, gps))\n",
    "    b.map(store, gps).compute()\n",
    "    print(ncfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n"
     ]
    }
   ],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## reload files and search for temporal and geographical proximity\n",
    "\n",
    "The parameter controling which pair will be selected is a distance `r_threshold`: any pair closer than this distance will be selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_threshold=500 # km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15329 single trajectories\n"
     ]
    }
   ],
   "source": [
    "files = sorted(glob(data_dir+'single/*.p'))\n",
    "#files = files[:500]\n",
    "print('%d single trajectories' %len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_traj(d1, iD, r_threshold):\n",
    "    D = [pickle.load(open(f, 'rb')) for f in files[iD]]\n",
    "    out = []\n",
    "    for d in D:\n",
    "        d2 = d[0]\n",
    "        try:\n",
    "            df = d1[0].join(d2, how='inner',lsuffix='_l',rsuffix='_r')\n",
    "            if not df.empty:\n",
    "                # compute distance\n",
    "                r = edr.haversine(df['LON_l'],df['LAT_l'], \n",
    "                              df['LON_r'],df['LAT_r'])\n",
    "                if not df[r<r_threshold].empty:\n",
    "                    out.append([d[1],d1[1]])\n",
    "        except:\n",
    "            out.append([d[1],d1[1],'error'])\n",
    "    if out:\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### profile with few pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.5 ms, sys: 2.95 ms, total: 28.5 ms\n",
      "Wall time: 25.9 ms\n",
      "CPU times: user 101 ms, sys: 3.94 ms, total: 105 ms\n",
      "Wall time: 103 ms\n",
      "CPU times: user 1min 27s, sys: 3.52 s, total: 1min 31s\n",
      "Wall time: 4min 12s\n"
     ]
    }
   ],
   "source": [
    "Nc = 10\n",
    "i=0\n",
    "iD = slice(i, min(i+Nc,len(files)))\n",
    "%time b = ( db.from_sequence(files[:], npartitions=1000) \\\n",
    "              .map(lambda f: pickle.load(open(f, 'rb'))) )\n",
    "%time lb = b.map(cross_traj, iD, r_threshold)\n",
    "%time p = lb.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.6 ms, sys: 29 Âµs, total: 21.6 ms\n",
      "Wall time: 18.9 ms\n",
      "CPU times: user 9.3 ms, sys: 1.04 ms, total: 10.3 ms\n",
      "Wall time: 8.49 ms\n",
      "CPU times: user 12.6 s, sys: 520 ms, total: 13.2 s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "iD = slice(i, min(i+Nc,len(files)))\n",
    "b = ( db.from_sequence(files[:],npartitions=100) \\\n",
    "              .map(lambda f: pickle.load(open(f, 'rb'))) )\n",
    "lb = b.map(cross_traj, iD, r_threshold)\n",
    "%time p = lb.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 85.1 ms, sys: 2.03 ms, total: 87.1 ms\n",
      "Wall time: 83.1 ms\n",
      "CPU times: user 12.5 ms, sys: 0 ns, total: 12.5 ms\n",
      "Wall time: 10.7 ms\n",
      "CPU times: user 1min 45s, sys: 4.03 s, total: 1min 49s\n",
      "Wall time: 8min 27s\n"
     ]
    }
   ],
   "source": [
    "Nc = 100\n",
    "iD = slice(i, min(i+Nc,len(files)))\n",
    "b = ( db.from_sequence(files[:],npartitions=100) \\\n",
    "              .map(lambda f: pickle.load(open(f, 'rb'))) )\n",
    "lb = b.map(cross_traj, iD, r_threshold)\n",
    "%time p = lb.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process all pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8100 / 15329\n",
      "8200 / 15329\n",
      "8300 / 15329\n",
      "8400 / 15329\n",
      "8500 / 15329\n",
      "8600 / 15329\n",
      "8700 / 15329\n",
      "8800 / 15329\n",
      "8900 / 15329\n",
      "9000 / 15329\n",
      "9100 / 15329\n",
      "9200 / 15329\n",
      "9300 / 15329\n",
      "9400 / 15329\n",
      "9500 / 15329\n",
      "9600 / 15329\n",
      "9700 / 15329\n",
      "9800 / 15329\n",
      "9900 / 15329\n",
      "10000 / 15329\n",
      "10100 / 15329\n",
      "10200 / 15329\n",
      "10300 / 15329\n",
      "10400 / 15329\n",
      "10500 / 15329\n",
      "10600 / 15329\n",
      "10700 / 15329\n",
      "10800 / 15329\n",
      "10900 / 15329\n",
      "11000 / 15329\n",
      "11100 / 15329\n",
      "11200 / 15329\n",
      "11300 / 15329\n",
      "11400 / 15329\n",
      "11500 / 15329\n",
      "11600 / 15329\n",
      "11700 / 15329\n",
      "11800 / 15329\n",
      "11900 / 15329\n",
      "12000 / 15329\n",
      "12100 / 15329\n",
      "12200 / 15329\n",
      "12300 / 15329\n",
      "12400 / 15329\n",
      "12500 / 15329\n",
      "12600 / 15329\n",
      "12700 / 15329\n",
      "12800 / 15329\n",
      "12900 / 15329\n",
      "13000 / 15329\n",
      "13100 / 15329\n",
      "13200 / 15329\n",
      "13300 / 15329\n",
      "13400 / 15329\n",
      "13500 / 15329\n",
      "13600 / 15329\n",
      "13700 / 15329\n",
      "13800 / 15329\n",
      "13900 / 15329\n",
      "14000 / 15329\n",
      "14100 / 15329\n",
      "14200 / 15329\n",
      "14300 / 15329\n",
      "14400 / 15329\n",
      "14500 / 15329\n",
      "14600 / 15329\n",
      "14700 / 15329\n",
      "14800 / 15329\n",
      "14900 / 15329\n",
      "15000 / 15329\n",
      "15100 / 15329\n",
      "15200 / 15329\n",
      "15300 / 15329\n"
     ]
    }
   ],
   "source": [
    "# loop around groups of drifters and\n",
    "Nc = 100\n",
    "i0 = 8100 # historic restart at 730\n",
    "for i in range(i0,len(files),Nc):\n",
    "    \n",
    "    file = data_dir+'pairs/%09d.p' %i\n",
    "    \n",
    "    if not os.path.isfile(file):\n",
    "\n",
    "        iD = slice(i, min(i+Nc,len(files)))\n",
    "\n",
    "        # load dask bag\n",
    "        b = ( db.from_sequence(files[i+1:], npartitions=100)\n",
    "              .map(lambda f: pickle.load(open(f, 'rb'))) )\n",
    "\n",
    "        # cross data\n",
    "        p = b.map(cross_traj, iD, r_threshold).compute()\n",
    "        p = [llp for lp in p if lp is not None for llp in lp]\n",
    "        p = [lp for lp in p if (lp[0]!=lp[1])]\n",
    "\n",
    "        pickle.dump( p, open( file, 'wb' ) , protocol=-1)\n",
    "        \n",
    "        #client.cancel(b)\n",
    "        #del b, p\n",
    "    \n",
    "    print('%d / %d'%(i,len(files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "equinox",
   "language": "python",
   "name": "equinox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
