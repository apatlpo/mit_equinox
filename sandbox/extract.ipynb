{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import dask\n",
    "#from dask_jobqueue import PBSCluster\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from mitequinox.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmethod = 1\n",
    "#\n",
    "if dmethod == 1:\n",
    "    from dask.distributed import Client\n",
    "    scheduler = os.getenv('DATAWORK')+'/dask/scheduler.json'\n",
    "    client = Client(scheduler_file=scheduler)\n",
    "elif dmethod == 2:\n",
    "    from dask_jobqueue import PBSCluster\n",
    "    # folder where data is spilled when RAM is filled up\n",
    "    local_dir = os.getenv('TMPDIR')\n",
    "    #\n",
    "    cluster = PBSCluster(queue='mpi_1', local_directory=local_dir, interface='ib0', walltime='24:00:00',\n",
    "                         threads=14, processes=2, memory='50GB', resource_spec='select=1:ncpus=28:mem=100g', \n",
    "                         death_timeout=100)\n",
    "    w = cluster.start_workers(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you need to wait for workers to spin up\n",
    "if dmethod == 2:\n",
    "    cluster.scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dask handles and check dask server status\n",
    "if dmethod == 2:\n",
    "    from dask.distributed import Client\n",
    "    client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# automatic extraction of all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rdir = '/home/datawork-lops-osi/data/mit4320/'\n",
    "grid_dir = data_rdir+'grid/'\n",
    "out_dir = '/home1/datawork/aponte/iwsst_mit/'\n",
    "\n",
    "def extract(v, t, face, i, j, pref=''):\n",
    "    \n",
    "    data_dir = data_rdir+v+'/'\n",
    "\n",
    "    iters, time = get_iters_time(v, data_dir, delta_t=25.)\n",
    "    \n",
    "    p = 'C'\n",
    "    if v is 'SSU':\n",
    "        p = 'W'\n",
    "    elif v is 'SSV':\n",
    "        p = 'S'\n",
    "    ds = get_compressed_data(v, data_dir, grid_dir, iters=iters, time=time, client=client, point=p)\n",
    "    ds = ds.chunk({'face': 1})\n",
    "    #print(ds)\n",
    "\n",
    "    file_out = out_dir+pref+v+'.nc'\n",
    "    ds[v].isel(time=t, face=face, i=i, j=j).to_netcdf(file_out, mode='w', unlimited_dims='time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = ['Eta', 'SST', 'SSU', 'SSV']\n",
    "#V = ['Eta', 'SST']\n",
    "#V = ['SSU']\n",
    "\n",
    "ext = {}\n",
    "ext['mad'] = {'time': slice(0,24), 'face': 1, 'i': slice(3000,None), 'j': slice(1000,4000)}\n",
    "ext['nwa'] = {'time': slice(0,24), 'face': 4, 'i': slice(2000,4000), 'j': slice(1500,3500)}\n",
    "\n",
    "for key, e in ext.items():\n",
    "    print('face = %d' %e['face'])\n",
    "    for v in V:\n",
    "        print(v)\n",
    "        extract(v, e['time'], e['face'], e['i'], e['j'], pref=key+'_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# manual exploration in order to get index slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = data_rdir+'Eta/'\n",
    "\n",
    "iters, time = get_iters_time('Eta', data_dir, delta_t=25.)\n",
    "\n",
    "ds = get_compressed_data('Eta', data_dir, grid_dir, iters=iters, time=time, client=client)\n",
    "ds = ds.chunk({'face': 1})\n",
    "print(ds)\n",
    "print('\\n data size: %.1f GB' %(ds['Eta'].nbytes / 1e9))\n",
    "print('\\n data size (1 face): %.1f GB' %(ds['Eta'].isel(face=1).nbytes / 1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['Eta'].isel(time=0, face=1).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['Eta'].isel(time=0, face=4).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## produce a netcdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.isel(time=slice(0,24), face=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_out = os.getenv('DATAWORK')+'/ssh_extract.nc'\n",
    "#ds.isel(time=slice(0,24), face=1).to_netcdf(file_out, mode='w', unlimited_dims='time') # not working, weird incompatible chunk error\n",
    "ds['Eta'].isel(time=slice(0,24), face=1).to_netcdf(file_out, mode='w', unlimited_dims='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
