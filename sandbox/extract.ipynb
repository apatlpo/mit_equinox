{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import dask\n",
    "#from dask_jobqueue import PBSCluster\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from mitequinox.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmethod = 1\n",
    "#\n",
    "if dmethod == 1:\n",
    "    from dask.distributed import Client\n",
    "    scheduler = os.getenv('DATAWORK')+'/dask/scheduler.json'\n",
    "    client = Client(scheduler_file=scheduler)\n",
    "elif dmethod == 2:\n",
    "    from dask_jobqueue import PBSCluster\n",
    "    # folder where data is spilled when RAM is filled up\n",
    "    local_dir = os.getenv('TMPDIR')\n",
    "    #\n",
    "    cluster = PBSCluster(queue='mpi_1', local_directory=local_dir, interface='ib0', walltime='24:00:00',\n",
    "                         threads=14, processes=2, memory='50GB', resource_spec='select=1:ncpus=28:mem=100g', \n",
    "                         death_timeout=100)\n",
    "    w = cluster.start_workers(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you need to wait for workers to spin up\n",
    "if dmethod == 2:\n",
    "    cluster.scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dask handles and check dask server status\n",
    "if dmethod == 2:\n",
    "    from dask.distributed import Client\n",
    "    client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# automatic extraction of all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rdir = '/home/datawork-lops-osi/data/mit4320/'\n",
    "grid_dir = data_rdir+'grid/'\n",
    "out_dir = '/home1/datawork/aponte/iwsst_mit/'\n",
    "\n",
    "def extract(v, t, face, i, j, pref=''):\n",
    "    \n",
    "    data_dir = data_rdir+v+'/'\n",
    "\n",
    "    iters, time = get_iters_time(v, data_dir, delta_t=25.)\n",
    "    \n",
    "    p = 'C'\n",
    "    if v is 'SSU':\n",
    "        p = 'W'\n",
    "    elif v is 'SSV':\n",
    "        p = 'S'\n",
    "    ds = get_compressed_data(v, data_dir, grid_dir, iters=iters, time=time, client=client, point=p)\n",
    "    ds = ds.chunk({'face': 1})\n",
    "    #print(ds)\n",
    "\n",
    "    file_out = out_dir+pref+v+'.nc'\n",
    "    ds[v].isel(time=t, face=face, i=i, j=j).to_netcdf(file_out, mode='w', unlimited_dims='time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = ['Eta', 'SST', 'SSU', 'SSV']\n",
    "#V = ['Eta', 'SST']\n",
    "#V = ['SSU']\n",
    "\n",
    "ext = {}\n",
    "ext['mad'] = {'time': slice(0,24), 'face': 1, 'i': slice(3000,None), 'j': slice(1000,4000)}\n",
    "ext['nwa'] = {'time': slice(0,24), 'face': 4, 'i': slice(2000,4000), 'j': slice(1500,3500)}\n",
    "\n",
    "for key, e in ext.items():\n",
    "    print('face = %d' %e['face'])\n",
    "    for v in V:\n",
    "        print(v)\n",
    "        extract(v, e['time'], e['face'], e['i'], e['j'], pref=key+'_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# manual exploration in order to get index slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = data_rdir+'Eta/'\n",
    "\n",
    "iters, time = get_iters_time('Eta', data_dir, delta_t=25.)\n",
    "\n",
    "ds = get_compressed_data('Eta', data_dir, grid_dir, iters=iters, time=time, client=client)\n",
    "ds = ds.chunk({'face': 1})\n",
    "print(ds)\n",
    "print('\\n data size: %.1f GB' %(ds['Eta'].nbytes / 1e9))\n",
    "print('\\n data size (1 face): %.1f GB' %(ds['Eta'].isel(face=1).nbytes / 1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['Eta'].isel(time=0, face=1).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['Eta'].isel(time=0, face=4).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## produce a netcdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.isel(time=slice(0,24), face=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_out = os.getenv('DATAWORK')+'/ssh_extract.nc'\n",
    "#ds.isel(time=slice(0,24), face=1).to_netcdf(file_out, mode='w', unlimited_dims='time') # not working, weird incompatible chunk error\n",
    "ds['Eta'].isel(time=slice(0,24), face=1).to_netcdf(file_out, mode='w', unlimited_dims='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# try to transpose data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nt = 24*10 # time windows to consider\n",
    "V = ['Eta', 'SST', 'SSS', 'SSU', 'SSV']\n",
    "V = ['Eta']\n",
    "out_dir = '/home1/datawork/aponte/mit_T/'\n",
    "\n",
    "#\n",
    "it = np.arange(time.size/Nt-1).astype(int)*Nt\n",
    "assert it[-1]+Nt<time.size\n",
    "\n",
    "for v in V:\n",
    "    #\n",
    "    data_dir = data_rdir+v+'/'\n",
    "    iters, time = get_iters_time(v, data_dir, delta_t=25.)\n",
    "    p = 'C'\n",
    "    if v is 'SSU':\n",
    "        p = 'W'\n",
    "    elif v is 'SSV':\n",
    "        p = 'S'\n",
    "    ds = get_compressed_data(v, data_dir, grid_dir, iters=iters, time=time, client=client, point=p)\n",
    "    ds = ds.chunk({'face': 1})\n",
    "    #\n",
    "    for face in ds['face']:\n",
    "        for i, t in enumerate(it):\n",
    "            dv = ds[v].isel(time=slice(t,t+Nt), face=face)\n",
    "            dv = dv.drop(['XC','YC','Depth','rA'])\n",
    "            dv = dv.transpose('i','j','time')\n",
    "            #\n",
    "            file_out = out_dir+'/%s_f%02d_t%02d.nc'%(v,face,i)\n",
    "            %time eta.to_netcdf(file_out, mode='w', unlimited_dims=['time'])            \n",
    "            #\n",
    "            print('face=%d / i=%d'%(face,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = ds['Eta'].isel(time=slice(0,24*10), face=1)\n",
    "print('\\n data size: %.1f GB' %(eta.nbytes / 1e9))\n",
    "print(eta)\n",
    "eta = eta.drop(['XC','YC','Depth','rA'])\n",
    "print(eta)\n",
    "eta = eta.transpose('i','j','time') # not lazy: \n",
    "print(eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_out = os.getenv('DATAWORK')+'/ssh_T0.nc'\n",
    "%time eta.to_netcdf(file_out, mode='w', unlimited_dims=['time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] 24 time steps, fast\n",
    "\n",
    "- [x] 24 time steps, slower \n",
    "\n",
    "```\n",
    "eta.to_netcdf(file_out, mode='w', unlimited_dims='time')\n",
    "\n",
    "netcdf ssh_T0 {\n",
    "dimensions:\n",
    "\tt = UNLIMITED ; // (0 currently)\n",
    "\ti = UNLIMITED ; // (4320 currently)\n",
    "\tm = UNLIMITED ; // (0 currently)\n",
    "\te = UNLIMITED ; // (0 currently)\n",
    "\tj = 4320 ;\n",
    "\ttime = UNLIMITED ; // (24 currently)\n",
    "variables:\n",
    "\tint64 i(i) ;\n",
    "\t\ti:standard_name = \"x_grid_index\" ;\n",
    "\t\ti:axis = \"X\" ;\n",
    "\t\ti:long_name = \"x-dimension of the t grid\" ;\n",
    "\t\ti:swap_dim = \"XC\" ;\n",
    "\tint64 j(j) ;\n",
    "\t\tj:standard_name = \"y_grid_index\" ;\n",
    "\t\tj:axis = \"Y\" ;\n",
    "\t\tj:long_name = \"y-dimension of the t grid\" ;\n",
    "\t\tj:swap_dim = \"YC\" ;\n",
    "\tint64 face ;\n",
    "\t\tface:standard_name = \"face_index\" ;\n",
    "\tdouble time(time) ;\n",
    "\t\ttime:_FillValue = NaN ;\n",
    "\tfloat Eta(i, j, time) ;\n",
    "\t\tEta:_FillValue = NaNf ;\n",
    "\t\tEta:coordinates = \"face\" ;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
