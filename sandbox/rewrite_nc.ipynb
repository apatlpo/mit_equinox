{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/cmocean/tools.py:76: MatplotlibDeprecationWarning: The is_string_like function was deprecated in version 2.1.\n",
      "  if not mpl.cbook.is_string_like(rgbin[0]):\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import dask\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from mitequinox.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/utils.py:128: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable\n",
      "  % (host, default, e), RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "dmethod = 2\n",
    "#\n",
    "if dmethod == 0:\n",
    "    client = None\n",
    "if dmethod == 1:\n",
    "    from dask.distributed import Client\n",
    "    scheduler = os.getenv('DATAWORK')+'/dask/scheduler.json'\n",
    "    client = Client(scheduler_file=scheduler)\n",
    "elif dmethod == 2:\n",
    "    from dask_jobqueue import PBSCluster\n",
    "    local_dir = os.getenv('TMPDIR')\n",
    "    cluster = PBSCluster(local_directory=local_dir)\n",
    "    #cluster = PBSCluster(local_directory=local_dir, threads=14, processes=1, memory='10GB')    \n",
    "    #print(cluster.job_script())\n",
    "    w = cluster.start_workers(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you need to wait for workers to spin up\n",
    "#if dmethod == 2:\n",
    "#    print(cluster.scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dask handles and check dask server status\n",
    "if dmethod == 2:\n",
    "    from dask.distributed import Client\n",
    "    client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://10.148.1.21:55439\n",
       "  <li><b>Dashboard: </b><a href='http://10.148.1.21:8787/status' target='_blank'>http://10.148.1.21:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>20</li>\n",
       "  <li><b>Cores: </b>80</li>\n",
       "  <li><b>Memory: </b>1000.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://10.148.1.21:55439' processes=20 cores=80>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "#PBS -N dask-worker\n",
      "#PBS -q mpi_1\n",
      "#PBS -A datarmor\n",
      "#PBS -l select=1:ncpus=28:mem=100GB\n",
      "#PBS -l walltime=24:00:00\n",
      "#PBS -m n\n",
      "\n",
      "\n",
      "\n",
      "/home1/datahome/aponte/.miniconda3/envs/equinox/bin/dask-worker tcp://10.148.1.131:54682 --nthreads 4 --nprocs 2 --memory-limit 50GB --name dask-worker-12 --death-timeout 60 --local-directory /dev/shm/pbs.1225796.datarmor0 --interface  ib0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cluster.job_script()) # original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "#PBS -N dask-worker\n",
      "#PBS -q mpi_1\n",
      "#PBS -A datarmor\n",
      "#PBS -l select=1:ncpus=28:mem=100GB\n",
      "#PBS -l walltime=24:00:00\n",
      "#PBS -m n\n",
      "\n",
      "\n",
      "\n",
      "/home1/datahome/aponte/.miniconda3/envs/equinox/bin/dask-worker tcp://10.148.1.21:55439 --nthreads 4 --nprocs 2 --memory-limit 50GB --name dask-worker-12 --death-timeout 60 --local-directory /dev/shm/pbs.1226550.datarmor0 --interface  ib0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cluster.job_script())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# store grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/xmitgcm/mds_store.py:721: UserWarning: Couldn't find available_diagnostics.log in . Using default version.\n",
      "  \"in %s. Using default version.\" % data_dir)\n",
      "/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/xmitgcm/utils.py:314: UserWarning: Not sure what to do with rlev = L\n",
      "  warnings.warn(\"Not sure what to do with rlev = \" + rlev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (face: 13, i: 4320, i_g: 4320, j: 4320, j_g: 4320, k: 90, k_l: 90, k_p1: 91, k_u: 90)\n",
      "Coordinates:\n",
      "  * i        (i) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ...\n",
      "  * i_g      (i_g) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ...\n",
      "  * j        (j) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ...\n",
      "  * j_g      (j_g) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ...\n",
      "  * k        (k) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ...\n",
      "  * k_u      (k_u) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ...\n",
      "  * k_l      (k_l) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ...\n",
      "  * k_p1     (k_p1) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ...\n",
      "  * face     (face) int64 0 1 2 3 4 5 6 7 8 9 10 11 12\n",
      "    XC       (face, j, i) >f4 dask.array<shape=(13, 4320, 4320), chunksize=(1, 4320, 4320)>\n",
      "    YC       (face, j, i) >f4 dask.array<shape=(13, 4320, 4320), chunksize=(1, 4320, 4320)>\n",
      "    XG       (face, j_g, i_g) >f4 dask.array<shape=(13, 4320, 4320), chunksize=(1, 4320, 4320)>\n",
      "    YG       (face, j_g, i_g) >f4 dask.array<shape=(13, 4320, 4320), chunksize=(1, 4320, 4320)>\n",
      "    rA       (face, j, i) >f4 dask.array<shape=(13, 4320, 4320), chunksize=(1, 4320, 4320)>\n",
      "    Depth    (face, j, i) >f4 dask.array<shape=(13, 4320, 4320), chunksize=(1, 4320, 4320)>\n",
      "Data variables:\n",
      "    *empty*\n",
      "Attributes:\n",
      "    Conventions:  CF-1.6\n",
      "    title:        netCDF wrapper of MITgcm MDS binary data\n",
      "    source:       MITgcm\n",
      "    history:      Created by calling `open_mdsdataset(llc_method='smallchunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/xmitgcm/mds_store.py:235: FutureWarning: iteration over an xarray.Dataset will change in xarray v0.11 to only include data variables, not coordinates. Iterate over the Dataset.variables property instead to preserve existing behavior in a forwards compatible manner.\n",
      "  for vname in ds:\n"
     ]
    }
   ],
   "source": [
    "ds_index, ds = get_compressed_level_index(grid_dir)\n",
    "\n",
    "ds = ds.drop(['dxG','dyG','dxC','dyC','rAw','rAs','rAz'])\n",
    "ds = ds.drop(['hFacC','hFacW','hFacS'])\n",
    "ds = ds.drop(['Z', 'Zp1', 'Zu', 'Zl', 'drC', 'drF','PHrefC','PHrefF'])\n",
    "print(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out_dir = scratch+'/'\n",
    "out_dir = osi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_out = out_dir+'mit_grid.nc'\n",
    "ds.to_netcdf(file_out, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (face: 13, i: 4320, i_g: 4320, j: 4320, j_g: 4320, k: 90, k_l: 90, k_p1: 91, k_u: 90)\n",
      "Coordinates:\n",
      "  * i        (i) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ...\n",
      "  * i_g      (i_g) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ...\n",
      "  * j        (j) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ...\n",
      "  * j_g      (j_g) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ...\n",
      "  * k        (k) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ...\n",
      "  * k_u      (k_u) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ...\n",
      "  * k_l      (k_l) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ...\n",
      "  * k_p1     (k_p1) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ...\n",
      "  * face     (face) int64 0 1 2 3 4 5 6 7 8 9 10 11 12\n",
      "    XC       (face, i, j) >f4 dask.array<shape=(13, 4320, 4320), chunksize=(1, 4320, 4320)>\n",
      "    YC       (face, i, j) >f4 dask.array<shape=(13, 4320, 4320), chunksize=(1, 4320, 4320)>\n",
      "    XG       (face, i_g, j_g) >f4 dask.array<shape=(13, 4320, 4320), chunksize=(1, 4320, 4320)>\n",
      "    YG       (face, i_g, j_g) >f4 dask.array<shape=(13, 4320, 4320), chunksize=(1, 4320, 4320)>\n",
      "    rA       (face, i, j) >f4 dask.array<shape=(13, 4320, 4320), chunksize=(1, 4320, 4320)>\n",
      "    Depth    (face, i, j) >f4 dask.array<shape=(13, 4320, 4320), chunksize=(1, 4320, 4320)>\n",
      "Data variables:\n",
      "    *empty*\n",
      "Attributes:\n",
      "    Conventions:  CF-1.6\n",
      "    title:        netCDF wrapper of MITgcm MDS binary data\n",
      "    source:       MITgcm\n",
      "    history:      Created by calling `open_mdsdataset(llc_method='smallchunks...\n"
     ]
    }
   ],
   "source": [
    "dst = ds.transpose('face', 'i', 'j', 'i_g', 'j_g', 'k', 'k_l', 'k_p1', 'k_u')\n",
    "print(dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_out = out_dir+'mit_grid_t.nc'\n",
    "dst.to_netcdf(file_out, mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# automatic rewriting of all variables\n",
    "\n",
    "# transposed data: (i,j,time), 1 file per face, time windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/xmitgcm/mds_store.py:721: UserWarning: Couldn't find available_diagnostics.log in . Using default version.\n",
      "  \"in %s. Using default version.\" % data_dir)\n",
      "/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/xmitgcm/utils.py:314: UserWarning: Not sure what to do with rlev = L\n",
      "  warnings.warn(\"Not sure what to do with rlev = \" + rlev)\n",
      "/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/xmitgcm/mds_store.py:235: FutureWarning: iteration over an xarray.Dataset will change in xarray v0.11 to only include data variables, not coordinates. Iterate over the Dataset.variables property instead to preserve existing behavior in a forwards compatible manner.\n",
      "  for vname in ds:\n"
     ]
    }
   ],
   "source": [
    "V = ['Eta', 'SST', 'SSS', 'SSU', 'SSV']\n",
    "V = ['Eta']\n",
    "\n",
    "transpose = True # False untested\n",
    "\n",
    "if transpose:\n",
    "    Nt = 24*10 # time windows to consider\n",
    "    #out_dir = scratch+'/mit_nc_t/'\n",
    "    #out_dir = datawork+'/mit_nc_t/'\n",
    "    out_dir = osi+'mit_nc_t/'\n",
    "    fsize_bound = 15*1e9\n",
    "else:\n",
    "    Nt = 1\n",
    "    #out_dir = datawork+'/mit_nc/'\n",
    "    #out_dir = scratch+'/mit_nc/'    \n",
    "    out_dir = osi+'mit_nc_t/'\n",
    "    fsize_bound = 60*1e6    \n",
    "\n",
    "for v in V:\n",
    "    #\n",
    "    data_dir = root_data_dir+v+'/'\n",
    "    iters, time = get_iters_time(v, data_dir, delta_t=25.)\n",
    "    #\n",
    "    it = np.arange(time.size/Nt-1).astype(int)*Nt\n",
    "    #it = np.arange(10).astype(int)*Nt # tmp\n",
    "    assert it[-1]+Nt<time.size\n",
    "    #\n",
    "    p = 'C'\n",
    "    if v is 'SSU':\n",
    "        p = 'W'\n",
    "    elif v is 'SSV':\n",
    "        p = 'S'\n",
    "    #\n",
    "    ds = get_compressed_data(v, data_dir, grid_dir, iters=iters, time=time, client=client, point=p)\n",
    "    #ds = ds.chunk({'face': 1})\n",
    "    #\n",
    "    #for face in [1]:\n",
    "    for face in range(ds['face'].size):\n",
    "        for i, t in enumerate(it):\n",
    "            #\n",
    "            file_out = out_dir+'/%s_f%02d_t%02d.nc'%(v,face,i)\n",
    "            if not os.path.isfile(file_out) or os.path.getsize(file_out) < fsize_bound:            \n",
    "                dv = ds[v].isel(time=slice(t,t+Nt), face=face) \n",
    "                # should store grid data independantly in a single file\n",
    "                dv = dv.drop(['XC','YC','Depth','rA']).to_dataset()\n",
    "                #\n",
    "                if transpose:\n",
    "                    dv = dv.chunk({'time': dv['time'].size, 'i': 432, 'j': 432})\n",
    "                    dv = dv.transpose('i','j','time')\n",
    "                    chunksizes = [432, 432, dv['time'].size]\n",
    "                else:\n",
    "                    dv = dv.chunk({'i': 432, 'j': 432})\n",
    "                    chunksizes = [1, 432, 432]\n",
    "                #print(dv)\n",
    "                #\n",
    "                dv = dv.load() # this was required in order to prevent pickle related errors\n",
    "                #\n",
    "                while True:\n",
    "                    try:\n",
    "                        %time dv.to_netcdf(file_out, mode='w', unlimited_dims=['time'], \\\n",
    "                                           encoding={'Eta': {'chunksizes': chunksizes}})\n",
    "                    except:\n",
    "                        print('Failure')\n",
    "                    if os.path.isfile(file_out) and os.path.getsize(file_out)>fsize_bound:\n",
    "                        #\n",
    "                        print('face=%d / i=%d'%(face,i))\n",
    "                        break\n",
    "            else:\n",
    "                print('face=%d / i=%d - allready processed'%(face,i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# standard data layout: (face, j, i), 1 file per time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = ['Eta', 'SST', 'SSS', 'SSU', 'SSV']\n",
    "V = ['Eta']\n",
    "\n",
    "Nt = 1\n",
    "#out_dir = datawork+'/mit_nc/'\n",
    "out_dir = scratch+'/mit_nc/'    \n",
    "fsize_bound = 13*60*1e6\n",
    "\n",
    "for v in V:\n",
    "    #\n",
    "    data_dir = root_data_dir+v+'/'\n",
    "    iters, time = get_iters_time(v, data_dir, delta_t=25.)\n",
    "    #\n",
    "    it = np.arange(time.size/Nt-1).astype(int)*Nt\n",
    "    #it = np.arange(10).astype(int)*Nt # tmp\n",
    "    assert it[-1]+Nt<time.size\n",
    "    #\n",
    "    p = 'C'\n",
    "    if v is 'SSU':\n",
    "        p = 'W'\n",
    "    elif v is 'SSV':\n",
    "        p = 'S'\n",
    "    #\n",
    "    ds = get_compressed_data(v, data_dir, grid_dir, iters=iters, time=time, client=client, point=p)\n",
    "    #ds = ds.chunk({'face': 1})\n",
    "    #\n",
    "    for i, t in enumerate(it):\n",
    "        #\n",
    "        file_out = out_dir+'/%s_t%04d.nc'%(v,i)\n",
    "        if not os.path.isfile(file_out) or os.path.getsize(file_out) < fsize_bound:            \n",
    "            dv = ds[v].isel(time=slice(t,t+Nt)) \n",
    "            # should store grid data independantly in a single file\n",
    "            dv = dv.drop(['XC','YC','Depth','rA']).to_dataset()\n",
    "            #\n",
    "            #print(dv)\n",
    "            #\n",
    "            while True:\n",
    "                try:\n",
    "                    #print(dv)\n",
    "                    %time dv.to_netcdf(file_out, mode='w')                    \n",
    "                except:\n",
    "                    print('Failure')\n",
    "                if os.path.isfile(file_out) and os.path.getsize(file_out) > fsize_bound:\n",
    "                    #\n",
    "                    print('i=%d, iter=%d'%(i, iters[i].values))\n",
    "                    break\n",
    "        else:\n",
    "            print('i=%d, iter=%d - allready processed'%(i, iters[i].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# preliminary attempts to transpose and store data in netcdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nt = 24*10 # time windows to consider\n",
    "v = 'Eta'\n",
    "face = 1\n",
    "out_dir = datawork+'/mit_tmp/'\n",
    "\n",
    "\n",
    "data_dir = root_data_dir+v+'/'\n",
    "iters, time = get_iters_time(v, data_dir, delta_t=25.)\n",
    "p = 'C'\n",
    "if v is 'SSU':\n",
    "    p = 'W'\n",
    "elif v is 'SSV':\n",
    "    p = 'S'\n",
    "ds = get_compressed_data(v, data_dir, grid_dir, iters=iters, time=time, client=client, point=p)\n",
    "ds = ds.chunk({'face': 1})\n",
    "\n",
    "dv = ds[v].isel(time=slice(0,Nt), face=face)\n",
    "\n",
    "# xarray stores datasets preferentially\n",
    "dv = dv.drop(['XC','YC','Depth','rA']).to_dataset()\n",
    "\n",
    "dv0 = dv\n",
    "print(dv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Default chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_out = out_dir+'/%s_f%02d_0.nc'%(v,face)\n",
    "%time dv.to_netcdf(file_out, mode='w', unlimited_dims=['time'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_eta = nc4.Dataset(file_out)['Eta']\n",
    "print(nc_eta)\n",
    "print(nc_eta.chunking())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### transposed dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "dv = dv0.transpose('i','j','time')\n",
    "print(dv)\n",
    "file_out = out_dir+'/%s_f%02d_1.nc'%(v,face)\n",
    "%time dv.to_netcdf(file_out, mode='w', unlimited_dims=['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_eta = nc4.Dataset(file_out)['Eta']\n",
    "print(nc_eta)\n",
    "print(nc_eta.chunking())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### transpose and rechunk\n",
    "\n",
    "the xarray rechunking does not affect the netcdf chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = dv0.chunk({'time': dv['time'].size})\n",
    "dv = dv.transpose('i','j','time')\n",
    "#dv = dv0.chunk({'time': dv['time'].size, 'i': 100, 'j': 100})\n",
    "print(dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_out = out_dir+'/%s_f%02d_2.nc'%(v,face)\n",
    "%time dv.to_netcdf(file_out, mode='w', unlimited_dims=['time'])\n",
    "#%time dv.to_netcdf(file_out, mode='w') # leads to contiguous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_eta = nc4.Dataset(file_out)['Eta']\n",
    "print(nc_eta)\n",
    "print(nc_eta.chunking())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### netcdf chunks passed with encoding option\n",
    "\n",
    "takes very long time, not good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_out = out_dir+'/%s_f%02d_3.nc'%(v,face)\n",
    "dv = dv0\n",
    "print(dv)\n",
    "print(dv['time'].size)\n",
    "#dv.to_netcdf(file_out, mode='w', encoding={'Eta': {'chunksizes': {'time': dv['time'].size, 'i': 432, 'j': 432}}})\n",
    "%time dv.to_netcdf(file_out, mode='w', unlimited_dims=['time'], \\\n",
    "             encoding={'Eta': {'chunksizes': [432, 432, dv['time'].size]}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_eta = nc4.Dataset(file_out)['Eta']\n",
    "print(nc_eta)\n",
    "print(nc_eta.chunking())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### same but with xarray rechunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dv = dv0.chunk({'time': dv['time'].size}) # 3min 46s\n",
    "dv = dv0.chunk({'time': dv['time'].size, 'i': 432, 'j': 432}) # 50.9 s\n",
    "dv = dv.transpose('i','j','time')\n",
    "print(dv)\n",
    "\n",
    "file_out = out_dir+'/%s_f%02d_4.nc'%(v,face)\n",
    "%time dv.to_netcdf(file_out, mode='w', unlimited_dims=['time'], \\\n",
    "             encoding={'Eta': {'chunksizes': [432, 432, dv['time'].size]}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nt = 24*10 # time windows to consider\n",
    "v = 'Eta'\n",
    "out_dir = datawork+'/mit_tmp/'\n",
    "\n",
    "\n",
    "data_dir = root_data_dir+v+'/'\n",
    "iters, time = get_iters_time(v, data_dir, delta_t=25.)\n",
    "p = 'C'\n",
    "if v is 'SSU':\n",
    "    p = 'W'\n",
    "elif v is 'SSV':\n",
    "    p = 'S'\n",
    "ds = get_compressed_data(v, data_dir, grid_dir, iters=iters, time=time, client=client, point=p)\n",
    "#ds = ds.chunk({'face': 1})\n",
    "\n",
    "it=22\n",
    "dv = ds.isel(time=it)\n",
    "#dv = ds[v].isel(time=it)\n",
    "\n",
    "# xarray stores datasets preferentially\n",
    "#dv = dv.drop(['XC','YC','Depth','rA']).to_dataset()\n",
    "dv = dv.drop(['XC','YC','XG','YG','Depth','rA'])\n",
    "dv = dv.drop(['dxG','dyG','dxC','dyC','rAw','rAs','rAz'])\n",
    "dv = dv.drop(['hFacC','hFacW','hFacS'])\n",
    "\n",
    "print('\\n data size: %.1f GB' %(dv.nbytes / 1e9))\n",
    "\n",
    "dv0 = dv\n",
    "print(dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = dv0\n",
    "file_out = out_dir+'%s_t%04d.nc'%(v, it)\n",
    "print(file_out)\n",
    "#%time dv.to_netcdf(file_out, mode='w', unlimited_dims=['time'])\n",
    "%time dv.to_netcdf(file_out, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.arange(4320)\n",
    "j = np.arange(4320)\n",
    "face = np.arange(13)\n",
    "v = xr.DataArray(np.random.randn(face.size, j.size, i.size), \\\n",
    "                  coords={'i': i, 'j': j, 'face': face}, dims=['face','j','i'])\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_out = out_dir+'rand.nc'\n",
    "%time dv.to_netcdf(file_out, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "```\n",
    "aponte/mit_tmp% ncdump -sh Eta_f01_4.nc\n",
    "\n",
    "netcdf Eta_f01_4 {\n",
    "dimensions:\n",
    "\ttime = UNLIMITED ; // (24 currently)\n",
    "\ti = 4320 ;\n",
    "\tj = 4320 ;\n",
    "variables:\n",
    "\tint64 i(i) ;\n",
    "\t\ti:standard_name = \"x_grid_index\" ;\n",
    "\t\ti:axis = \"X\" ;\n",
    "\t\ti:long_name = \"x-dimension of the t grid\" ;\n",
    "\t\ti:swap_dim = \"XC\" ;\n",
    "\t\ti:_Storage = \"contiguous\" ;\n",
    "\t\ti:_Endianness = \"little\" ;\n",
    "\tint64 j(j) ;\n",
    "\t\tj:standard_name = \"y_grid_index\" ;\n",
    "\t\tj:axis = \"Y\" ;\n",
    "\t\tj:long_name = \"y-dimension of the t grid\" ;\n",
    "\t\tj:swap_dim = \"YC\" ;\n",
    "\t\tj:_Storage = \"contiguous\" ;\n",
    "\t\tj:_Endianness = \"little\" ;\n",
    "\tint64 face ;\n",
    "\t\tface:standard_name = \"face_index\" ;\n",
    "\t\tface:_Endianness = \"little\" ;\n",
    "\tdouble time(time) ;\n",
    "\t\ttime:_FillValue = NaN ;\n",
    "\t\ttime:_Storage = \"chunked\" ;\n",
    "\t\ttime:_ChunkSizes = 512 ;\n",
    "\tfloat Eta(i, j, time) ;\n",
    "\t\tEta:_FillValue = NaNf ;\n",
    "\t\tEta:coordinates = \"face\" ;\n",
    "\t\tEta:_Storage = \"chunked\" ;\n",
    "\t\tEta:_ChunkSizes = 432, 432, 24 ;\n",
    "\n",
    "// global attributes:\n",
    "\t\t:_NCProperties = \"version=1|netcdflibversion=4.6.1|hdf5libversion=1.10.1\" ;\n",
    "\t\t:_Format = \"netCDF-4\" ;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.stop_workers(cluster.jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
