{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.7/site-packages/distributed/utils.py:137: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable\n",
      "  RuntimeWarning,\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#from mitequinox.utils import *\n",
    "from mitequinox.binary import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:54278</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>8</li>\n",
       "  <li><b>Cores: </b>56</li>\n",
       "  <li><b>Memory: </b>107.37 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:54278' processes=8 threads=56, memory=107.37 GB>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "#\n",
    "cluster = LocalCluster()\n",
    "#\n",
    "#from dask_jobqueue import PBSCluster\n",
    "#cluster = PBSCluster()\n",
    "#w = cluster.scale(28*1)\n",
    "#\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/datawork-lops-osi/equinox/mit4320/zarr/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dir = osi+'equinox/mit4320/zarr/'\n",
    "out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zarr compression\n",
    "# http://xarray.pydata.org/en/stable/io.html\n",
    "# http://zarr.readthedocs.io/en/stable/tutorial.html#compressors\n",
    "#compressor = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# store grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.7/site-packages/xmitgcm/mds_store.py:837: UserWarning: Couldn't find available_diagnostics.log in  or /home/datawork-lops-osi/equinox/mit4320/grid/. Using default version.\n",
      "  \"in %s or %s. Using default version.\" % (data_dir, grid_dir))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (face: 13, i: 4320, i_g: 4320, j: 4320, j_g: 4320, k: 90, k_l: 90, k_p1: 91, k_u: 90)\n",
      "Coordinates:\n",
      "  * i        (i) int64 0 1 2 3 4 5 6 7 ... 4313 4314 4315 4316 4317 4318 4319\n",
      "  * i_g      (i_g) int64 0 1 2 3 4 5 6 7 ... 4313 4314 4315 4316 4317 4318 4319\n",
      "  * j        (j) int64 0 1 2 3 4 5 6 7 ... 4313 4314 4315 4316 4317 4318 4319\n",
      "  * j_g      (j_g) int64 0 1 2 3 4 5 6 7 ... 4313 4314 4315 4316 4317 4318 4319\n",
      "  * k        (k) int64 0 1 2 3 4 5 6 7 8 9 10 ... 80 81 82 83 84 85 86 87 88 89\n",
      "  * k_u      (k_u) int64 0 1 2 3 4 5 6 7 8 9 ... 80 81 82 83 84 85 86 87 88 89\n",
      "  * k_l      (k_l) int64 0 1 2 3 4 5 6 7 8 9 ... 80 81 82 83 84 85 86 87 88 89\n",
      "  * k_p1     (k_p1) int64 0 1 2 3 4 5 6 7 8 9 ... 81 82 83 84 85 86 87 88 89 90\n",
      "  * face     (face) int64 0 1 2 3 4 5 6 7 8 9 10 11 12\n",
      "    XC       (face, j, i) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
      "    YC       (face, j, i) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
      "    XG       (face, j_g, i_g) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
      "    YG       (face, j_g, i_g) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
      "    CS       (face, j, i) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
      "    SN       (face, j, i) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
      "    Z        (k) >f4 -0.5 -1.57 -2.79 -4.185 ... -5881.88 -6301.185 -6760.17\n",
      "    Zp1      (k_p1) >f4 0.0 -1.0 -2.14 -3.44 ... -6082.07 -6520.3 -7000.04\n",
      "    Zu       (k_u) >f4 -1.0 -2.14 -3.44 -4.93 ... -6082.07 -6520.3 -7000.04\n",
      "    Zl       (k_l) >f4 0.0 -1.0 -2.14 -3.44 ... -5681.69 -6082.07 -6520.3\n",
      "    rA       (face, j, i) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
      "    dxG      (face, j_g, i) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
      "    dyG      (face, j, i_g) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
      "    Depth    (face, j, i) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
      "    rAz      (face, j_g, i_g) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
      "    dxC      (face, j, i_g) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
      "    dyC      (face, j_g, i) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
      "    rAw      (face, j, i_g) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
      "    rAs      (face, j_g, i) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
      "    drC      (k_p1) >f4 0.5 1.07 1.22 1.395 ... 383.155 419.305 458.985 239.87\n",
      "    drF      (k) >f4 1.0 1.14 1.3 1.49 1.7 ... 365.93 400.38 438.23 479.74\n",
      "    PHrefC   (k) >f4 4.905 15.4017 27.3699 ... 57701.242 61814.625 66317.266\n",
      "    PHrefF   (k_p1) >f4 0.0 9.81 20.9934 ... 59665.105 63964.145 68670.39\n",
      "    hFacC    (k, face, j, i) >f4 dask.array<chunksize=(1, 1, 4320, 4320), meta=np.ndarray>\n",
      "    hFacW    (k, face, j, i_g) >f4 dask.array<chunksize=(1, 1, 4320, 4320), meta=np.ndarray>\n",
      "    hFacS    (k, face, j_g, i) >f4 dask.array<chunksize=(1, 1, 4320, 4320), meta=np.ndarray>\n",
      "    maskC    (k, face, j, i) bool dask.array<chunksize=(1, 1, 4320, 4320), meta=np.ndarray>\n",
      "    maskW    (k, face, j, i_g) bool dask.array<chunksize=(1, 1, 4320, 4320), meta=np.ndarray>\n",
      "    maskS    (k, face, j_g, i) bool dask.array<chunksize=(1, 1, 4320, 4320), meta=np.ndarray>\n",
      "Data variables:\n",
      "    *empty*\n",
      "Attributes:\n",
      "    Conventions:  CF-1.6\n",
      "    title:        netCDF wrapper of MITgcm MDS binary data\n",
      "    source:       MITgcm\n",
      "    history:      Created by calling `open_mdsdataset(grid_dir='/home/datawor...\n"
     ]
    }
   ],
   "source": [
    "ds_index, ds = get_compressed_level_index(grid_dir)\n",
    "\n",
    "# we will need to keep some of these when computing gradients\n",
    "#ds = ds.drop(['dxG','dyG','dxC','dyC','rAw','rAs','rAz'])\n",
    "#ds = ds.drop(['hFacC','hFacW','hFacS'])\n",
    "#ds = ds.drop(['maskC','maskW','maskS'])\n",
    "#ds = ds.drop(['Z', 'Zp1', 'Zu', 'Zl', 'drC', 'drF','PHrefC','PHrefF'])\n",
    "\n",
    "#Nc = 432 # original choice\n",
    "#Nc = 27 # very long scheduling\n",
    "#Nc = 96 # 96*45\n",
    "#ds = ds.chunk({'i': Nc, 'j': Nc, 'i_g': Nc, 'j_g': Nc})\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this way too slow, this may be optimized at xmitgcm calls\n",
    "ds = ds.isel(k=0).persist()\n",
    "#ds.rAw.isel(face=1).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_out = out_dir+'grid.zarr'\n",
    "%time ds.to_zarr(file_out, mode='w')\n",
    "file_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# !!! do not look beyond this point !!!\n",
    "\n",
    "# standard data layout: chunks (face, time, j, i) = (1, 1, 4320, 4320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#V = ['Eta', 'SST', 'SSS', 'SSU', 'SSV']\n",
    "V = ['oceTAUX']\n",
    "#V = ['oceTAUX', 'oceTAUY', 'KPPhbl']\n",
    "\n",
    "\n",
    "# scheduler does not like (i.e. takes a long time to do anything) any spatial rechunking on top \n",
    "# of that on faces:\n",
    "#Nc = 480 # x 9 = 4320\n",
    "#Nc = 96 # x 45 = 4320\n",
    "# other choices: 432, 27 (long scheduling), 288, 864\n",
    "\n",
    "#out_dir = scratch+'/mit/standard/'\n",
    "#scratchd = '/home/c11-data/Test_aponte/'\n",
    "#out_dir = scratchd+'/mit/standard/'\n",
    "\n",
    "for v in V:\n",
    "    #\n",
    "    data_dir = root_data_dir+v+'/'\n",
    "    # !!! should be removed eventually\n",
    "    data_dir = root_data_dir+'tmp/'\n",
    "    # !!!    \n",
    "    iters, time = get_iters_time(v, data_dir, delta_t=25.)\n",
    "    #\n",
    "    p = 'C'\n",
    "    if v in ['SSU','oceTAUX']:\n",
    "        p = 'W'\n",
    "    elif v is ['SSV','oceTAUY']:\n",
    "        p = 'S'\n",
    "    #\n",
    "    ds = get_compressed_data(v, data_dir, grid_dir, iters=iters, time=time, client=client, point=p)\n",
    "    #\n",
    "    # should store grid data independantly in a single file\n",
    "    ds = ds.drop(['XC','YC','Depth','rA'])\n",
    "    #\n",
    "    #ds = ds.isel(time=slice(1000))\n",
    "    #ds = ds.chunk({'face': 1})\n",
    "    #ds = ds.chunk({'face': 1, 'i': Nc, 'j': Nc}) # scheduler does not like this\n",
    "    #\n",
    "    dv = ds[v].to_dataset()\n",
    "    #\n",
    "    #dv = dv.chunk({'i': Nc, 'j': Nc}) # scheduler does not like this either\n",
    "    #dv = dv.isel(time=0)\n",
    "    #\n",
    "    file_out = out_dir+'%s.zarr'%(v)\n",
    "    try:\n",
    "        #print(dv)\n",
    "        %time dv.to_zarr(file_out, mode='w')                    \n",
    "    except:\n",
    "        print('Failure')\n",
    "    dsize = getsize(file_out)\n",
    "    print('   data is %.1fGB ' %(dsize/1e9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# try to load standard lay out, rechunk and store right away\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for rechunking\n",
    "\n",
    "Nt = 24*10 # time chunks\n",
    "#Nt = 0\n",
    "#\n",
    "Nt = len(ds.time) if Nt == 0 else Nt\n",
    "\n",
    "Nc = 96 # x 45 = 4320\n",
    "# other choices: 432, 27 (long scheduling), 288, 864"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one face at a time, all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 24s, sys: 23.9 s, total: 8min 48s\n",
      "Wall time: 14min 49s\n",
      " SSU face=1  data is 628.6GB \n"
     ]
    }
   ],
   "source": [
    "# same but over all variables and faces\n",
    "#V = ['SSU', 'SSV']\n",
    "V = ['SSU']\n",
    "\n",
    "out_dir = scratch+'mit/rechunked/'\n",
    "\n",
    "for v in V:\n",
    "\n",
    "    file_in = scratch+'/mit/standard/%s.zarr'%(v)\n",
    "    ds0 = xr.open_zarr(file_in)\n",
    "    \n",
    "    #for face in range(ds0['face'].size):\n",
    "    for face in [1]:\n",
    "        \n",
    "        ds = ds0.isel(face=face)\n",
    "        #\n",
    "        ds = ds.isel(time=slice(len(ds.time)//Nt *Nt))\n",
    "        #\n",
    "        ds = ds.chunk({'time': Nt, 'i': Nc, 'j': Nc})\n",
    "        #\n",
    "        # tmp, xarray zarr backend bug: \n",
    "        # https://github.com/pydata/xarray/issues/2278\n",
    "        del ds['face'].encoding['chunks']\n",
    "        del ds[v].encoding['chunks']\n",
    "        \n",
    "        file_out = out_dir+'%s_f%02d.zarr'%(v,face)\n",
    "        try:\n",
    "            #%time ds.to_zarr(file_out, mode='w')\n",
    "            # specify compression:\n",
    "            %time ds.to_zarr(file_out, mode='w', \\\n",
    "                             encoding={key: {'compressor': compressor} for key in ds.variables})\n",
    "            # without compression: 601G for face 1\n",
    "        except:\n",
    "            print('Failure')\n",
    "        dsize = getsize(file_out)\n",
    "        print(' %s face=%d  data is %.1fGB ' %(v, face, dsize/1e9))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = cluster.start_workers(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - ERROR - Not all workers responded positively: ['timed out', 'timed out', 'timed out', 'timed out', 'timed out', 'timed out', 'timed out', 'timed out', 'timed out', 'timed out', 'timed out', 'timed out', 'timed out', 'timed out', 'timed out', 'timed out', 'timed out', 'timed out', 'timed out', 'timed out']\n",
      "NoneType: None\n",
      "distributed.client - ERROR - Restart timed out after 20.000000 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://10.148.1.12:42619\n",
       "  <li><b>Dashboard: </b><a href='http://10.148.1.12:8787/status' target='_blank'>http://10.148.1.12:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://10.148.1.12:42619' processes=0 cores=0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - ERROR - '1846309'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846309'\n",
      "distributed.scheduler - ERROR - '1846313'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846313'\n",
      "distributed.scheduler - ERROR - '1846313'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846313'\n",
      "distributed.scheduler - ERROR - '1846309'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846309'\n",
      "distributed.scheduler - ERROR - '1846314'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846314'\n",
      "distributed.scheduler - ERROR - '1846308'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846308'\n",
      "distributed.scheduler - ERROR - '1846314'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846314'\n",
      "distributed.scheduler - ERROR - '1846308'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846308'\n",
      "distributed.scheduler - ERROR - '1846307'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846307'\n",
      "distributed.scheduler - ERROR - '1846311'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846311'\n",
      "distributed.scheduler - ERROR - '1846311'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846311'\n",
      "distributed.scheduler - ERROR - '1846305'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846305'\n",
      "distributed.scheduler - ERROR - '1846307'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846307'\n",
      "distributed.scheduler - ERROR - '1846305'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846305'\n",
      "distributed.scheduler - ERROR - '1846312'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846312'\n",
      "distributed.scheduler - ERROR - '1846312'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846312'\n",
      "distributed.scheduler - ERROR - '1846306'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846306'\n",
      "distributed.scheduler - ERROR - '1846306'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846306'\n",
      "distributed.scheduler - ERROR - '1846310'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846310'\n",
      "distributed.scheduler - ERROR - '1846310'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846310'\n"
     ]
    }
   ],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "distributed.utils - ERROR - \n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.7/site-packages/distributed/utils.py\", line 666, in log_errors\n",
      "    yield\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.7/site-packages/distributed/client.py\", line 1283, in _close\n",
      "    await gen.with_timeout(timedelta(seconds=2), list(coroutines))\n",
      "concurrent.futures._base.CancelledError\n",
      "distributed.utils - ERROR - \n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.7/site-packages/distributed/utils.py\", line 666, in log_errors\n",
      "    yield\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.7/site-packages/distributed/client.py\", line 1012, in _reconnect\n",
      "    await self._close()\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.7/site-packages/distributed/client.py\", line 1283, in _close\n",
      "    await gen.with_timeout(timedelta(seconds=2), list(coroutines))\n",
      "concurrent.futures._base.CancelledError\n"
     ]
    }
   ],
   "source": [
    "# kill scheduler, workers\n",
    "cluster.close()\n",
    "#cluster.stop_workers(cluster.jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
