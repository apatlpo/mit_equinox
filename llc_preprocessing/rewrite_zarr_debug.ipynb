{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.7/site-packages/distributed/utils.py:137: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable\n",
      "  RuntimeWarning,\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from mitequinox.utils import *\n",
    "#from mitequinox.binary import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:46121</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>8</li>\n",
       "  <li><b>Cores: </b>56</li>\n",
       "  <li><b>Memory: </b>107.37 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:46121' processes=8 threads=56, memory=107.37 GB>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "#\n",
    "cluster = LocalCluster()\n",
    "#\n",
    "#from dask_jobqueue import PBSCluster\n",
    "#cluster = PBSCluster()\n",
    "#w = cluster.scale(28*1)\n",
    "#\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from dask_jobqueue import PBSCluster\n",
    "#cluster = PBSCluster()\n",
    "#cluster = PBSCluster(cores=4, processes=2, memory='100G', resource_spec='select=1:ncpus=28:mem=110GB') #\n",
    "#w = cluster.scale(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/datawork-lops-osi/equinox/mit4320/zarr/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dir = osi+'equinox/mit4320/zarr/'\n",
    "out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zarr compression\n",
    "# http://xarray.pydata.org/en/stable/io.html\n",
    "# http://zarr.readthedocs.io/en/stable/tutorial.html#compressors\n",
    "#compressor = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# debug "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import xmitgcm as xm\n",
    "#\n",
    "def get_compressed_level_index(grid_dir, \n",
    "                               index_fname='llc4320_compressed_level_index.nc', \n",
    "                               geometry='llc'):\n",
    "    ''' Some doc\n",
    "    '''\n",
    "    #\n",
    "    ds = xm.open_mdsdataset('', grid_dir=grid_dir,\n",
    "                             iters=None, geometry=geometry, read_grid=True,\n",
    "                             default_dtype=np.dtype('>f4'),\n",
    "                             ignore_unknown_vars=True)\n",
    "    \n",
    "    # get shape\n",
    "    #nz, nface, ny, nx = ds.hFacC.shape\n",
    "    #shape = (1, nface, ny, nx)\n",
    "    \n",
    "    try:\n",
    "        ds_index = xr.open_dataset(grid_dir+index_fname)\n",
    "    except OSError:\n",
    "        # compute and save mask indices\n",
    "        print('Create llc4320_compressed_level_index.nc in grid_dir')\n",
    "        ds_index = ((ds.reset_coords()[['hFacC', 'hFacW','hFacS']] > 0).sum(axis=(1, 2, 3)))\n",
    "        ds_index.coords['k'] = ds.k\n",
    "        ds_index.load().to_netcdf(grid_dir+index_fname)\n",
    "        print('done')\n",
    "\n",
    "    return ds_index, ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.7/site-packages/xmitgcm/mds_store.py:837: UserWarning: Couldn't find available_diagnostics.log in  or /home/datawork-lops-osi/equinox/mit4320/grid/. Using default version.\n",
      "  \"in %s or %s. Using default version.\" % (data_dir, grid_dir))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.Dataset&gt;\n",
       "Dimensions:  (face: 13, i: 4320, i_g: 4320, j: 4320, j_g: 4320, k: 90, k_l: 90, k_p1: 91, k_u: 90)\n",
       "Coordinates:\n",
       "  * i        (i) int64 0 1 2 3 4 5 6 7 ... 4313 4314 4315 4316 4317 4318 4319\n",
       "  * i_g      (i_g) int64 0 1 2 3 4 5 6 7 ... 4313 4314 4315 4316 4317 4318 4319\n",
       "  * j        (j) int64 0 1 2 3 4 5 6 7 ... 4313 4314 4315 4316 4317 4318 4319\n",
       "  * j_g      (j_g) int64 0 1 2 3 4 5 6 7 ... 4313 4314 4315 4316 4317 4318 4319\n",
       "  * k        (k) int64 0 1 2 3 4 5 6 7 8 9 10 ... 80 81 82 83 84 85 86 87 88 89\n",
       "  * k_u      (k_u) int64 0 1 2 3 4 5 6 7 8 9 ... 80 81 82 83 84 85 86 87 88 89\n",
       "  * k_l      (k_l) int64 0 1 2 3 4 5 6 7 8 9 ... 80 81 82 83 84 85 86 87 88 89\n",
       "  * k_p1     (k_p1) int64 0 1 2 3 4 5 6 7 8 9 ... 81 82 83 84 85 86 87 88 89 90\n",
       "  * face     (face) int64 0 1 2 3 4 5 6 7 8 9 10 11 12\n",
       "    XC       (face, j, i) &gt;f4 dask.array&lt;chunksize=(1, 4320, 4320), meta=np.ndarray&gt;\n",
       "    YC       (face, j, i) &gt;f4 dask.array&lt;chunksize=(1, 4320, 4320), meta=np.ndarray&gt;\n",
       "    XG       (face, j_g, i_g) &gt;f4 dask.array&lt;chunksize=(1, 4320, 4320), meta=np.ndarray&gt;\n",
       "    YG       (face, j_g, i_g) &gt;f4 dask.array&lt;chunksize=(1, 4320, 4320), meta=np.ndarray&gt;\n",
       "    CS       (face, j, i) &gt;f4 dask.array&lt;chunksize=(1, 4320, 4320), meta=np.ndarray&gt;\n",
       "    SN       (face, j, i) &gt;f4 dask.array&lt;chunksize=(1, 4320, 4320), meta=np.ndarray&gt;\n",
       "    Z        (k) &gt;f4 -0.5 -1.57 -2.79 -4.185 ... -5881.88 -6301.185 -6760.17\n",
       "    Zp1      (k_p1) &gt;f4 0.0 -1.0 -2.14 -3.44 ... -6082.07 -6520.3 -7000.04\n",
       "    Zu       (k_u) &gt;f4 -1.0 -2.14 -3.44 -4.93 ... -6082.07 -6520.3 -7000.04\n",
       "    Zl       (k_l) &gt;f4 0.0 -1.0 -2.14 -3.44 ... -5681.69 -6082.07 -6520.3\n",
       "    rA       (face, j, i) &gt;f4 dask.array&lt;chunksize=(1, 4320, 4320), meta=np.ndarray&gt;\n",
       "    dxG      (face, j_g, i) &gt;f4 dask.array&lt;chunksize=(1, 4320, 4320), meta=np.ndarray&gt;\n",
       "    dyG      (face, j, i_g) &gt;f4 dask.array&lt;chunksize=(1, 4320, 4320), meta=np.ndarray&gt;\n",
       "    Depth    (face, j, i) &gt;f4 dask.array&lt;chunksize=(1, 4320, 4320), meta=np.ndarray&gt;\n",
       "    rAz      (face, j_g, i_g) &gt;f4 dask.array&lt;chunksize=(1, 4320, 4320), meta=np.ndarray&gt;\n",
       "    dxC      (face, j, i_g) &gt;f4 dask.array&lt;chunksize=(1, 4320, 4320), meta=np.ndarray&gt;\n",
       "    dyC      (face, j_g, i) &gt;f4 dask.array&lt;chunksize=(1, 4320, 4320), meta=np.ndarray&gt;\n",
       "    rAw      (face, j, i_g) &gt;f4 dask.array&lt;chunksize=(1, 4320, 4320), meta=np.ndarray&gt;\n",
       "    rAs      (face, j_g, i) &gt;f4 dask.array&lt;chunksize=(1, 4320, 4320), meta=np.ndarray&gt;\n",
       "    drC      (k_p1) &gt;f4 0.5 1.07 1.22 1.395 ... 383.155 419.305 458.985 239.87\n",
       "    drF      (k) &gt;f4 1.0 1.14 1.3 1.49 1.7 ... 365.93 400.38 438.23 479.74\n",
       "    PHrefC   (k) &gt;f4 4.905 15.4017 27.3699 ... 57701.242 61814.625 66317.266\n",
       "    PHrefF   (k_p1) &gt;f4 0.0 9.81 20.9934 ... 59665.105 63964.145 68670.39\n",
       "    hFacC    (k, face, j, i) &gt;f4 dask.array&lt;chunksize=(1, 1, 4320, 4320), meta=np.ndarray&gt;\n",
       "    hFacW    (k, face, j, i_g) &gt;f4 dask.array&lt;chunksize=(1, 1, 4320, 4320), meta=np.ndarray&gt;\n",
       "    hFacS    (k, face, j_g, i) &gt;f4 dask.array&lt;chunksize=(1, 1, 4320, 4320), meta=np.ndarray&gt;\n",
       "    maskC    (k, face, j, i) bool dask.array&lt;chunksize=(1, 1, 4320, 4320), meta=np.ndarray&gt;\n",
       "    maskW    (k, face, j, i_g) bool dask.array&lt;chunksize=(1, 1, 4320, 4320), meta=np.ndarray&gt;\n",
       "    maskS    (k, face, j_g, i) bool dask.array&lt;chunksize=(1, 1, 4320, 4320), meta=np.ndarray&gt;\n",
       "Data variables:\n",
       "    *empty*\n",
       "Attributes:\n",
       "    Conventions:  CF-1.6\n",
       "    title:        netCDF wrapper of MITgcm MDS binary data\n",
       "    source:       MITgcm\n",
       "    history:      Created by calling `open_mdsdataset(grid_dir=&#x27;/home/datawor...</pre>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (face: 13, i: 4320, i_g: 4320, j: 4320, j_g: 4320, k: 90, k_l: 90, k_p1: 91, k_u: 90)\n",
       "Coordinates:\n",
       "  * i        (i) int64 0 1 2 3 4 5 6 7 ... 4313 4314 4315 4316 4317 4318 4319\n",
       "  * i_g      (i_g) int64 0 1 2 3 4 5 6 7 ... 4313 4314 4315 4316 4317 4318 4319\n",
       "  * j        (j) int64 0 1 2 3 4 5 6 7 ... 4313 4314 4315 4316 4317 4318 4319\n",
       "  * j_g      (j_g) int64 0 1 2 3 4 5 6 7 ... 4313 4314 4315 4316 4317 4318 4319\n",
       "  * k        (k) int64 0 1 2 3 4 5 6 7 8 9 10 ... 80 81 82 83 84 85 86 87 88 89\n",
       "  * k_u      (k_u) int64 0 1 2 3 4 5 6 7 8 9 ... 80 81 82 83 84 85 86 87 88 89\n",
       "  * k_l      (k_l) int64 0 1 2 3 4 5 6 7 8 9 ... 80 81 82 83 84 85 86 87 88 89\n",
       "  * k_p1     (k_p1) int64 0 1 2 3 4 5 6 7 8 9 ... 81 82 83 84 85 86 87 88 89 90\n",
       "  * face     (face) int64 0 1 2 3 4 5 6 7 8 9 10 11 12\n",
       "    XC       (face, j, i) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
       "    YC       (face, j, i) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
       "    XG       (face, j_g, i_g) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
       "    YG       (face, j_g, i_g) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
       "    CS       (face, j, i) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
       "    SN       (face, j, i) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
       "    Z        (k) >f4 -0.5 -1.57 -2.79 -4.185 ... -5881.88 -6301.185 -6760.17\n",
       "    Zp1      (k_p1) >f4 0.0 -1.0 -2.14 -3.44 ... -6082.07 -6520.3 -7000.04\n",
       "    Zu       (k_u) >f4 -1.0 -2.14 -3.44 -4.93 ... -6082.07 -6520.3 -7000.04\n",
       "    Zl       (k_l) >f4 0.0 -1.0 -2.14 -3.44 ... -5681.69 -6082.07 -6520.3\n",
       "    rA       (face, j, i) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
       "    dxG      (face, j_g, i) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
       "    dyG      (face, j, i_g) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
       "    Depth    (face, j, i) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
       "    rAz      (face, j_g, i_g) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
       "    dxC      (face, j, i_g) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
       "    dyC      (face, j_g, i) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
       "    rAw      (face, j, i_g) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
       "    rAs      (face, j_g, i) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
       "    drC      (k_p1) >f4 0.5 1.07 1.22 1.395 ... 383.155 419.305 458.985 239.87\n",
       "    drF      (k) >f4 1.0 1.14 1.3 1.49 1.7 ... 365.93 400.38 438.23 479.74\n",
       "    PHrefC   (k) >f4 4.905 15.4017 27.3699 ... 57701.242 61814.625 66317.266\n",
       "    PHrefF   (k_p1) >f4 0.0 9.81 20.9934 ... 59665.105 63964.145 68670.39\n",
       "    hFacC    (k, face, j, i) >f4 dask.array<chunksize=(1, 1, 4320, 4320), meta=np.ndarray>\n",
       "    hFacW    (k, face, j, i_g) >f4 dask.array<chunksize=(1, 1, 4320, 4320), meta=np.ndarray>\n",
       "    hFacS    (k, face, j_g, i) >f4 dask.array<chunksize=(1, 1, 4320, 4320), meta=np.ndarray>\n",
       "    maskC    (k, face, j, i) bool dask.array<chunksize=(1, 1, 4320, 4320), meta=np.ndarray>\n",
       "    maskW    (k, face, j, i_g) bool dask.array<chunksize=(1, 1, 4320, 4320), meta=np.ndarray>\n",
       "    maskS    (k, face, j_g, i) bool dask.array<chunksize=(1, 1, 4320, 4320), meta=np.ndarray>\n",
       "Data variables:\n",
       "    *empty*\n",
       "Attributes:\n",
       "    Conventions:  CF-1.6\n",
       "    title:        netCDF wrapper of MITgcm MDS binary data\n",
       "    source:       MITgcm\n",
       "    history:      Created by calling `open_mdsdataset(grid_dir='/home/datawor..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_index, ds = get_compressed_level_index(grid_dir)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.isel(k=0).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_vars_datashrunk = ['Eta', 'oceTAUX', 'oceTAUY', 'KPPhbl']\n",
    "\n",
    "def load_level_from_3D_field(data_dir, varname, inum, offset, count, mask, dtype):\n",
    "    ''' Some doc\n",
    "    '''\n",
    "\n",
    "    # all iters in one directory:\n",
    "    inum_str = '%010d' % inum\n",
    "    if varname in _vars_datashrunk:\n",
    "        suff = '.data.shrunk'\n",
    "    else:\n",
    "        suff = '.shrunk'            \n",
    "    fname = os.path.join(data_dir, '%s.%s' % (varname, inum_str) +suff)\n",
    "    \n",
    "    with open(fname, mode='rb') as file:\n",
    "        file.seek(offset * dtype.itemsize)\n",
    "        data = np.fromfile(file, dtype=dtype, count=count)\n",
    "    \n",
    "    data_blank = np.full_like(mask, np.nan, dtype='f4')\n",
    "    data_blank[mask] = data\n",
    "    data_blank.shape = mask.shape\n",
    "    data_llc = xm.utils._reshape_llc_data(data_blank, jdim=0).compute(scheduler=dask.get)\n",
    "    data_llc.shape = (1,) + data_llc.shape\n",
    "    return data_llc\n",
    "\n",
    "\n",
    "def lazily_load_level_from_3D_field(data_dir, varname, inum, offset, count, mask, shape, dtype):\n",
    "    ''' Some doc\n",
    "    '''\n",
    "    return dask.array.from_delayed(dask.delayed(load_level_from_3D_field)\n",
    "                            (data_dir, varname, inum, offset, count, mask, dtype),\n",
    "                            shape, dtype)\n",
    "\n",
    "\n",
    "def get_compressed_data(varname, data_dir, grid_dir, ds_index=None, ds=None, iters='all', \n",
    "                        time=None, client=None, k=0, point='C', **kwargs):\n",
    "    ''' Get mitgcm compressed data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    varname: string\n",
    "        Variable name to load (should allow for list?)\n",
    "    data_dir: string\n",
    "        Path to the directory where the mds .data and .meta files are stored\n",
    "    grid_dir: string\n",
    "        Path to the directory where grid files are stored\n",
    "    ds_index: xarray Dataset, optional\n",
    "        Contains compressed file\n",
    "    iters: list, 'all', optional\n",
    "        The iterations numbers of the files to be read. If 'all' (default), all iterations \n",
    "        will be read.\n",
    "    k: int\n",
    "        vertical level loaded\n",
    "    point: string\n",
    "        grid point used for the mask\n",
    "    '''\n",
    "    dtype = np.dtype('>f4')\n",
    "    \n",
    "    if ds_index is None or shape is None or ds is None:\n",
    "        ds_index, ds = get_compressed_level_index(grid_dir, **kwargs)\n",
    "        # get shape\n",
    "        nz, nface, ny, nx = ds.hFacC.shape\n",
    "        shape = (1, nface, ny, nx)        \n",
    "        \n",
    "    strides = [0,] + list(ds_index['hFac' + point].data)\n",
    "    offset = strides[k]\n",
    "    count = strides[k+1]\n",
    "    \n",
    "    if iters is 'all':\n",
    "        iters = xm.mds_store._get_all_iternums(data_dir, file_prefixes=varname, \n",
    "                                               file_format='*.??????????.data.shrunk')\n",
    "        \n",
    "    # load mask from raw data\n",
    "    hfac = xm.utils.read_mds(grid_dir + 'hFac' + point, llc=True, dtype=dtype,\n",
    "                             use_mmap=False, use_dask=True, chunks='2D')['hFac'+point][k].compute()\n",
    "    #hfac = xm.utils.read_mds(grid_dir + 'hFac' + point,\n",
    "    #                         use_mmap=True, use_dask=False, chunks=\"2D\") #, force_dict=False) \n",
    "    #hfac = xm.utils.read_mds(grid_dir + 'hFac' + point, llc=True,\n",
    "    #                         use_mmap=True, use_dask=False, extra_metadata=llc4320)['hFac' + point]\n",
    "    #                        use_mmap=True, use_dask=False, force_dict=False)['hFac' + point]\n",
    "    #hfac = xm.utils.read_3d_llc_data(grid_dir + 'hFac'+point+'.data', 90, 4320, dtype='>f4', memmap=True)\n",
    "    #mask = hfac[k]>0\n",
    "    mask = hfac>0\n",
    "    if client is None:\n",
    "        mask_future = mask\n",
    "    else:\n",
    "        mask_future = client.scatter(mask)\n",
    "    \n",
    "    data = dask.array.concatenate([lazily_load_level_from_3D_field\n",
    "                            (data_dir, varname, i, offset, count, mask_future, shape, dtype)\n",
    "                            for i in iters], axis=0)\n",
    "\n",
    "    if point is 'C':\n",
    "        dims = ['time', 'face', 'j', 'i']\n",
    "    elif point is 'W':\n",
    "        dims = ['time', 'face', 'j', 'i_g']\n",
    "    elif point is 'S':\n",
    "        dims = ['time', 'face', 'j_g', 'i']    \n",
    "    \n",
    "    ds[varname] = xr.Variable(dims, data)   \n",
    "    \n",
    "    if time is not None:\n",
    "        ds['time'] = time.sel(iters=iters).values\n",
    "        #ds['dtime'] = iters_to_date(iters)\n",
    "        ds = ds.assign_coords(dtime=xr.DataArray(iters_to_date(iters), dims=['time']))\n",
    "        #ds = ds.assign(dtime=)\n",
    "        \n",
    "    return ds\n",
    "\n",
    "\n",
    "def get_iters_time(varname, data_dir, delta_t=25.):\n",
    "    ''' get iteration numbers and derives corresponding time\n",
    "    Parameters\n",
    "    ----------\n",
    "    varname: string\n",
    "        Variable name to load (should allow for list?)\n",
    "    data_dir: string\n",
    "        Path to the directory where the mds .data and .meta files are stored\n",
    "    delta_t: float\n",
    "        Model time step\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    iters: xarray DataArray\n",
    "        iteration numbers indexed by time\n",
    "    time: xarray DataArray\n",
    "        time in seconds\n",
    "    '''\n",
    "    file_suff = '.shrunk'\n",
    "    if varname in _vars_datashrunk:\n",
    "        file_suff = '.data.shrunk'\n",
    "    #\n",
    "    iters = xm.mds_store._get_all_iternums(data_dir, file_prefixes=varname, \n",
    "                                           file_format='*.??????????'+file_suff)\n",
    "    time = delta_t * np.array(iters)\n",
    "    \n",
    "    iters = xr.DataArray(iters, coords=[time], dims=['time'])\n",
    "    time = xr.DataArray(time, coords=[iters.values], dims=['iters'])\n",
    "    \n",
    "    return iters, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.7/site-packages/xmitgcm/mds_store.py:837: UserWarning: Couldn't find available_diagnostics.log in  or /home/datawork-lops-osi/equinox/mit4320/grid/. Using default version.\n",
      "  \"in %s or %s. Using default version.\" % (data_dir, grid_dir))\n"
     ]
    }
   ],
   "source": [
    "#V = ['Eta', 'SST', 'SSS', 'SSU', 'SSV']\n",
    "V = ['oceTAUX', 'oceTAUY', 'KPPhbl'] # need to move files\n",
    "\n",
    "# scheduler does not like (i.e. takes a long time to do anything) any spatial rechunking on top \n",
    "# of that on faces:\n",
    "#Nc = 480 # x 9 = 4320\n",
    "#Nc = 96 # x 45 = 4320\n",
    "# other choices: 432, 27 (long scheduling), 288, 864\n",
    "\n",
    "#out_dir = scratch+'/mit/standard/'\n",
    "#scratchd = '/home/c11-data/Test_aponte/'\n",
    "#out_dir = scratchd+'/mit/standard/'\n",
    "\n",
    "# !!!\n",
    "v = V[0]\n",
    "\n",
    "#\n",
    "data_dir = root_data_dir+'bin/'+v+'/'\n",
    "# !!! should be removed eventually\n",
    "data_dir = root_data_dir+'tmp/'\n",
    "# !!!\n",
    "iters, time = get_iters_time(v, data_dir, delta_t=25.)\n",
    "#\n",
    "p = 'C'\n",
    "if v in ['SSU','oceTAUX']:\n",
    "    p = 'W'\n",
    "elif v is ['SSV','oceTAUY']:\n",
    "    p = 'S'\n",
    "#\n",
    "ds = get_compressed_data(v, data_dir, grid_dir, iters=iters, time=time, client=client, point=p)\n",
    "#\n",
    "# should store grid data independantly in a single file\n",
    "ds = ds.drop(['XC','YC','Depth','rA'])\n",
    "#\n",
    "#ds = ds.isel(time=slice(1000))\n",
    "#ds = ds.chunk({'face': 1})\n",
    "#ds = ds.chunk({'face': 1, 'i': Nc, 'j': Nc}) # scheduler does not like this\n",
    "#\n",
    "dv = ds[v].to_dataset()\n",
    "\n",
    "# !!! tmp\n",
    "dv = dv.isel(time=slice(0,10))\n",
    "#\n",
    "\n",
    "#\n",
    "#dv = dv.chunk({'i': Nc, 'j': Nc}) # scheduler does not like this either\n",
    "#\n",
    "file_out = out_dir+'%s.zarr'%(v)\n",
    "#try:\n",
    "#    print(dv)\n",
    "#    %time dv.to_zarr(file_out, mode='w')\n",
    "#except:\n",
    "#    print('Failure')\n",
    "#dsize = getsize(file_out)\n",
    "#print('   data is %.1fGB ' %(dsize/1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.Dataset&gt;\n",
       "Dimensions:  (face: 13, i_g: 4320, j: 4320, time: 10)\n",
       "Coordinates:\n",
       "  * i_g      (i_g) int64 0 1 2 3 4 5 6 7 ... 4313 4314 4315 4316 4317 4318 4319\n",
       "  * j        (j) int64 0 1 2 3 4 5 6 7 ... 4313 4314 4315 4316 4317 4318 4319\n",
       "  * face     (face) int64 0 1 2 3 4 5 6 7 8 9 10 11 12\n",
       "    dyG      (face, j, i_g) &gt;f4 dask.array&lt;chunksize=(1, 4320, 4320), meta=np.ndarray&gt;\n",
       "    dxC      (face, j, i_g) &gt;f4 dask.array&lt;chunksize=(1, 4320, 4320), meta=np.ndarray&gt;\n",
       "    rAw      (face, j, i_g) &gt;f4 dask.array&lt;chunksize=(1, 4320, 4320), meta=np.ndarray&gt;\n",
       "  * time     (time) float64 2.592e+05 2.628e+05 2.664e+05 ... 2.88e+05 2.916e+05\n",
       "    dtime    (time) datetime64[ns] 2011-09-13 ... 2011-09-13T09:00:00\n",
       "Data variables:\n",
       "    oceTAUX  (time, face, j, i_g) float32 dask.array&lt;chunksize=(1, 13, 4320, 4320), meta=np.ndarray&gt;</pre>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (face: 13, i_g: 4320, j: 4320, time: 10)\n",
       "Coordinates:\n",
       "  * i_g      (i_g) int64 0 1 2 3 4 5 6 7 ... 4313 4314 4315 4316 4317 4318 4319\n",
       "  * j        (j) int64 0 1 2 3 4 5 6 7 ... 4313 4314 4315 4316 4317 4318 4319\n",
       "  * face     (face) int64 0 1 2 3 4 5 6 7 8 9 10 11 12\n",
       "    dyG      (face, j, i_g) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
       "    dxC      (face, j, i_g) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
       "    rAw      (face, j, i_g) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
       "  * time     (time) float64 2.592e+05 2.628e+05 2.664e+05 ... 2.88e+05 2.916e+05\n",
       "    dtime    (time) datetime64[ns] 2011-09-13 ... 2011-09-13T09:00:00\n",
       "Data variables:\n",
       "    oceTAUX  (time, face, j, i_g) float32 dask.array<chunksize=(1, 13, 4320, 4320), meta=np.ndarray>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.backends.zarr.ZarrStore at 0x2aaaaf52ac50>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv.to_zarr(file_out, mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# store grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/datahome/aponte/xmitgcm/xmitgcm/mds_store.py:740: UserWarning: Couldn't find available_diagnostics.log in . Using default version.\n",
      "  \"in %s. Using default version.\" % data_dir)\n",
      "/home1/datahome/aponte/xmitgcm/xmitgcm/utils.py:336: UserWarning: Not sure what to do with rlev = L\n",
      "  warnings.warn(\"Not sure what to do with rlev = \" + rlev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (face: 13, i: 4320, i_g: 4320, j: 4320, j_g: 4320, k: 90, k_l: 90, k_p1: 91, k_u: 90)\n",
      "Coordinates:\n",
      "  * i        (i) int64 0 1 2 3 4 5 6 7 ... 4313 4314 4315 4316 4317 4318 4319\n",
      "  * i_g      (i_g) int64 0 1 2 3 4 5 6 7 ... 4313 4314 4315 4316 4317 4318 4319\n",
      "  * j        (j) int64 0 1 2 3 4 5 6 7 ... 4313 4314 4315 4316 4317 4318 4319\n",
      "  * j_g      (j_g) int64 0 1 2 3 4 5 6 7 ... 4313 4314 4315 4316 4317 4318 4319\n",
      "  * k        (k) int64 0 1 2 3 4 5 6 7 8 9 10 ... 80 81 82 83 84 85 86 87 88 89\n",
      "  * k_u      (k_u) int64 0 1 2 3 4 5 6 7 8 9 ... 80 81 82 83 84 85 86 87 88 89\n",
      "  * k_l      (k_l) int64 0 1 2 3 4 5 6 7 8 9 ... 80 81 82 83 84 85 86 87 88 89\n",
      "  * k_p1     (k_p1) int64 0 1 2 3 4 5 6 7 8 9 ... 81 82 83 84 85 86 87 88 89 90\n",
      "  * face     (face) int64 0 1 2 3 4 5 6 7 8 9 10 11 12\n",
      "    XC       (face, j, i) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
      "    YC       (face, j, i) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
      "    XG       (face, j_g, i_g) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
      "    YG       (face, j_g, i_g) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
      "    CS       (face, j, i) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
      "    SN       (face, j, i) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
      "    Z        (k) >f4 -0.5 -1.57 -2.79 -4.185 ... -5881.88 -6301.185 -6760.17\n",
      "    Zp1      (k_p1) >f4 0.0 -1.0 -2.14 -3.44 ... -6082.07 -6520.3 -7000.04\n",
      "    Zu       (k_u) >f4 -1.0 -2.14 -3.44 -4.93 ... -6082.07 -6520.3 -7000.04\n",
      "    Zl       (k_l) >f4 0.0 -1.0 -2.14 -3.44 ... -5681.69 -6082.07 -6520.3\n",
      "    rA       (face, j, i) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
      "    dxG      (face, j_g, i) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
      "    dyG      (face, j, i_g) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
      "    Depth    (face, j, i) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
      "    rAz      (face, j_g, i_g) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
      "    dxC      (face, j, i_g) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
      "    dyC      (face, j_g, i) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
      "    rAw      (face, j, i_g) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
      "    rAs      (face, j_g, i) >f4 dask.array<chunksize=(1, 4320, 4320), meta=np.ndarray>\n",
      "    drC      (k_p1) >f4 0.5 1.07 1.22 1.395 ... 383.155 419.305 458.985 239.87\n",
      "    drF      (k) >f4 1.0 1.14 1.3 1.49 1.7 ... 365.93 400.38 438.23 479.74\n",
      "    PHrefC   (k) >f4 4.905 15.4017 27.3699 ... 57701.242 61814.625 66317.266\n",
      "    PHrefF   (k_p1) >f4 0.0 9.81 20.9934 ... 59665.105 63964.145 68670.39\n",
      "    hFacC    (k, face, j, i) >f4 dask.array<chunksize=(1, 1, 4320, 4320), meta=np.ndarray>\n",
      "    hFacW    (k, face, j, i_g) >f4 dask.array<chunksize=(1, 1, 4320, 4320), meta=np.ndarray>\n",
      "    hFacS    (k, face, j_g, i) >f4 dask.array<chunksize=(1, 1, 4320, 4320), meta=np.ndarray>\n",
      "Data variables:\n",
      "    *empty*\n",
      "Attributes:\n",
      "    Conventions:  CF-1.6\n",
      "    title:        netCDF wrapper of MITgcm MDS binary data\n",
      "    source:       MITgcm\n",
      "    history:      Created by calling `open_mdsdataset(grid_dir='/home/datawor...\n"
     ]
    }
   ],
   "source": [
    "ds_index, ds = get_compressed_level_index(grid_dir)\n",
    "\n",
    "# we will need to keep some of these when computing gradients\n",
    "#ds = ds.drop(['dxG','dyG','dxC','dyC','rAw','rAs','rAz'])\n",
    "#ds = ds.drop(['hFacC','hFacW','hFacS'])\n",
    "#ds = ds.drop(['maskC','maskW','maskS'])\n",
    "#ds = ds.drop(['Z', 'Zp1', 'Zu', 'Zl', 'drC', 'drF','PHrefC','PHrefF'])\n",
    "\n",
    "#Nc = 432 # original choice\n",
    "#Nc = 27 # very long scheduling\n",
    "#Nc = 96 # 96*45\n",
    "#ds = ds.chunk({'i': Nc, 'j': Nc, 'i_g': Nc, 'j_g': Nc})\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this way too slow, this may be optimized at xmitgcm calls\n",
    "ds = ds.isel(k=0).persist()\n",
    "#ds.rAw.isel(face=1).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_out = out_dir+'grid.zarr'\n",
    "%time ds.to_zarr(file_out, mode='w')\n",
    "file_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# standard data layout: chunks (face, time, j, i) = (1, 1, 4320, 4320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/xmitgcm/mds_store.py:735: UserWarning: Couldn't find available_diagnostics.log in . Using default version.\n",
      "  \"in %s. Using default version.\" % data_dir)\n",
      "/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/xmitgcm/utils.py:336: UserWarning: Not sure what to do with rlev = L\n",
      "  warnings.warn(\"Not sure what to do with rlev = \" + rlev)\n",
      "/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/xmitgcm/mds_store.py:249: FutureWarning: iteration over an xarray.Dataset will change in xarray v0.11 to only include data variables, not coordinates. Iterate over the Dataset.variables property instead to preserve existing behavior in a forwards compatible manner.\n",
      "  for vname in ds:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (face: 13, i_g: 4320, j: 4320, time: 1000)\n",
      "Coordinates:\n",
      "  * i_g      (i_g) int64 0 1 2 3 4 5 6 7 ... 4313 4314 4315 4316 4317 4318 4319\n",
      "  * j        (j) int64 0 1 2 3 4 5 6 7 ... 4313 4314 4315 4316 4317 4318 4319\n",
      "  * face     (face) int64 0 1 2 3 4 5 6 7 8 9 10 11 12\n",
      "    dyG      (face, j, i_g) >f4 dask.array<shape=(13, 4320, 4320), chunksize=(1, 4320, 4320)>\n",
      "    dxC      (face, j, i_g) >f4 dask.array<shape=(13, 4320, 4320), chunksize=(1, 4320, 4320)>\n",
      "    rAw      (face, j, i_g) >f4 dask.array<shape=(13, 4320, 4320), chunksize=(1, 4320, 4320)>\n",
      "  * time     (time) float64 5.702e+06 5.706e+06 5.71e+06 ... 9.295e+06 9.299e+06\n",
      "    dtime    (time) datetime64[ns] 2011-11-15 ... 2011-12-26T15:00:00\n",
      "Data variables:\n",
      "    SSU      (time, face, j, i_g) >f4 dask.array<shape=(1000, 13, 4320, 4320), chunksize=(1, 13, 4320, 4320)>\n",
      "CPU times: user 1min 7s, sys: 3.44 s, total: 1min 10s\n",
      "Wall time: 12min 49s\n",
      "   data is 526.7GB \n"
     ]
    }
   ],
   "source": [
    "#V = ['Eta', 'SST', 'SSS', 'SSU', 'SSV']\n",
    "V = ['oceTAUX', 'oceTAUY', 'KPPhbl']\n",
    "\n",
    "# scheduler does not like (i.e. takes a long time to do anything) any spatial rechunking on top \n",
    "# of that on faces:\n",
    "#Nc = 480 # x 9 = 4320\n",
    "#Nc = 96 # x 45 = 4320\n",
    "# other choices: 432, 27 (long scheduling), 288, 864\n",
    "\n",
    "#out_dir = scratch+'/mit/standard/'\n",
    "#scratchd = '/home/c11-data/Test_aponte/'\n",
    "#out_dir = scratchd+'/mit/standard/'\n",
    "\n",
    "for v in V:\n",
    "    #\n",
    "    data_dir = root_data_dir+v+'/'\n",
    "    # !!! should be removed eventually\n",
    "    data_dir = root_data_dir+'tmp/'\n",
    "    # !!!    \n",
    "    iters, time = get_iters_time(v, data_dir, delta_t=25.)\n",
    "    #\n",
    "    p = 'C'\n",
    "    if v in ['SSU','oceTAUX']:\n",
    "        p = 'W'\n",
    "    elif v is ['SSV','oceTAUY']:\n",
    "        p = 'S'\n",
    "    #\n",
    "    ds = get_compressed_data(v, data_dir, grid_dir, iters=iters, time=time, client=client, point=p)\n",
    "    #\n",
    "    # should store grid data independantly in a single file\n",
    "    ds = ds.drop(['XC','YC','Depth','rA'])\n",
    "    #\n",
    "    #ds = ds.isel(time=slice(1000))\n",
    "    #ds = ds.chunk({'face': 1})\n",
    "    #ds = ds.chunk({'face': 1, 'i': Nc, 'j': Nc}) # scheduler does not like this\n",
    "    #\n",
    "    dv = ds[v].to_dataset()\n",
    "    #\n",
    "    #dv = dv.chunk({'i': Nc, 'j': Nc}) # scheduler does not like this either\n",
    "    #\n",
    "    file_out = out_dir+'%s.zarr'%(v)\n",
    "    try:\n",
    "        #print(dv)\n",
    "        %time dv.to_zarr(file_out, mode='w')                    \n",
    "    except:\n",
    "        print('Failure')\n",
    "    dsize = getsize(file_out)\n",
    "    print('   data is %.1fGB ' %(dsize/1e9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# try to load standard lay out, rechunk and store right away\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for rechunking\n",
    "\n",
    "Nt = 24*10 # time chunks\n",
    "#Nt = 0\n",
    "#\n",
    "Nt = len(ds.time) if Nt == 0 else Nt\n",
    "\n",
    "Nc = 96 # x 45 = 4320\n",
    "# other choices: 432, 27 (long scheduling), 288, 864"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one face at a time, all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 24s, sys: 23.9 s, total: 8min 48s\n",
      "Wall time: 14min 49s\n",
      " SSU face=1  data is 628.6GB \n"
     ]
    }
   ],
   "source": [
    "# same but over all variables and faces\n",
    "#V = ['SSU', 'SSV']\n",
    "V = ['SSU']\n",
    "\n",
    "out_dir = scratch+'mit/rechunked/'\n",
    "\n",
    "for v in V:\n",
    "\n",
    "    file_in = scratch+'/mit/standard/%s.zarr'%(v)\n",
    "    ds0 = xr.open_zarr(file_in)\n",
    "    \n",
    "    #for face in range(ds0['face'].size):\n",
    "    for face in [1]:\n",
    "        \n",
    "        ds = ds0.isel(face=face)\n",
    "        #\n",
    "        ds = ds.isel(time=slice(len(ds.time)//Nt *Nt))\n",
    "        #\n",
    "        ds = ds.chunk({'time': Nt, 'i': Nc, 'j': Nc})\n",
    "        #\n",
    "        # tmp, xarray zarr backend bug: \n",
    "        # https://github.com/pydata/xarray/issues/2278\n",
    "        del ds['face'].encoding['chunks']\n",
    "        del ds[v].encoding['chunks']\n",
    "        \n",
    "        file_out = out_dir+'%s_f%02d.zarr'%(v,face)\n",
    "        try:\n",
    "            #%time ds.to_zarr(file_out, mode='w')\n",
    "            # specify compression:\n",
    "            %time ds.to_zarr(file_out, mode='w', \\\n",
    "                             encoding={key: {'compressor': compressor} for key in ds.variables})\n",
    "            # without compression: 601G for face 1\n",
    "        except:\n",
    "            print('Failure')\n",
    "        dsize = getsize(file_out)\n",
    "        print(' %s face=%d  data is %.1fGB ' %(v, face, dsize/1e9))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = cluster.start_workers(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - ERROR - Not all workers responded positively: ['timed out', 'timed out', 'timed out', 'timed out', 'timed out', 'timed out', 'timed out', 'timed out', 'timed out', 'timed out', 'timed out', 'timed out', 'timed out', 'timed out', 'timed out', 'timed out', 'timed out', 'timed out', 'timed out', 'timed out']\n",
      "NoneType: None\n",
      "distributed.client - ERROR - Restart timed out after 20.000000 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://10.148.1.12:42619\n",
       "  <li><b>Dashboard: </b><a href='http://10.148.1.12:8787/status' target='_blank'>http://10.148.1.12:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://10.148.1.12:42619' processes=0 cores=0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - ERROR - '1846309'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846309'\n",
      "distributed.scheduler - ERROR - '1846313'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846313'\n",
      "distributed.scheduler - ERROR - '1846313'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846313'\n",
      "distributed.scheduler - ERROR - '1846309'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846309'\n",
      "distributed.scheduler - ERROR - '1846314'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846314'\n",
      "distributed.scheduler - ERROR - '1846308'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846308'\n",
      "distributed.scheduler - ERROR - '1846314'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846314'\n",
      "distributed.scheduler - ERROR - '1846308'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846308'\n",
      "distributed.scheduler - ERROR - '1846307'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846307'\n",
      "distributed.scheduler - ERROR - '1846311'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846311'\n",
      "distributed.scheduler - ERROR - '1846311'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846311'\n",
      "distributed.scheduler - ERROR - '1846305'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846305'\n",
      "distributed.scheduler - ERROR - '1846307'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846307'\n",
      "distributed.scheduler - ERROR - '1846305'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846305'\n",
      "distributed.scheduler - ERROR - '1846312'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846312'\n",
      "distributed.scheduler - ERROR - '1846312'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846312'\n",
      "distributed.scheduler - ERROR - '1846306'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846306'\n",
      "distributed.scheduler - ERROR - '1846306'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846306'\n",
      "distributed.scheduler - ERROR - '1846310'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846310'\n",
      "distributed.scheduler - ERROR - '1846310'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/distributed/scheduler.py\", line 1306, in add_worker\n",
      "    plugin.add_worker(scheduler=self, worker=address)\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.6/site-packages/dask_jobqueue/core.py\", line 62, in add_worker\n",
      "    self.running_jobs[job_id] = self.pending_jobs.pop(job_id)\n",
      "KeyError: '1846310'\n"
     ]
    }
   ],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "distributed.utils - ERROR - \n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.7/site-packages/distributed/utils.py\", line 666, in log_errors\n",
      "    yield\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.7/site-packages/distributed/client.py\", line 1283, in _close\n",
      "    await gen.with_timeout(timedelta(seconds=2), list(coroutines))\n",
      "concurrent.futures._base.CancelledError\n",
      "distributed.utils - ERROR - \n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.7/site-packages/distributed/utils.py\", line 666, in log_errors\n",
      "    yield\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.7/site-packages/distributed/client.py\", line 1012, in _reconnect\n",
      "    await self._close()\n",
      "  File \"/home1/datahome/aponte/.miniconda3/envs/equinox/lib/python3.7/site-packages/distributed/client.py\", line 1283, in _close\n",
      "    await gen.with_timeout(timedelta(seconds=2), list(coroutines))\n",
      "concurrent.futures._base.CancelledError\n"
     ]
    }
   ],
   "source": [
    "# kill scheduler, workers\n",
    "cluster.close()\n",
    "#cluster.stop_workers(cluster.jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
